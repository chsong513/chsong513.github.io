<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>支持向量机</title>
    <url>/20171014_SVM/index.html</url>
    <content><![CDATA[<p><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1507893605639&di=16fad19f6589ac2b2dc8cd4c7d74f1f3&imgtype=0&src=http%3A%2F%2Fsigvc.org%2Fwhy%2Fbook%2F3dp%2Fpaste2.files%2Fimage004.jpg" alt=""></p>
<p>支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。</p>
<a id="more"></a>

<h2 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h2><p>支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。</p>
<h3 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h3><p>支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$<br>$$<br>\hat \gamma_i = y_i(\omega\cdot x_i + b) \tag{1}<br>$$<br>然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$</p>
<p>几何间隔由空间中两点的实际距离定义记作$\gamma_i$<br>$$<br>\gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}\tag{2}<br>$$<br>同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$</p>
<p>可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x + b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x + 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。</p>
<p>同时还可以看出函数间隔和几何间隔之间满足关系(3)<br>$$<br>\gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3}<br>$$<br>因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题<br>$$<br>\max_{\omega,b} \quad \gamma\tag{4}\\<br>s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N<br>$$<br>根据上述关系(3)将问题转换为<br>$$<br>\max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\<br>s.t.\quad y_i(\omega\cdot x_i + b) \ge \hat\gamma,\qquad i = 1,2,\cdots,N<br>$$<br>因为函数距离跟随$\omega$和$b$而变化，这里做出限制$\hat\gamma = 1$。而最大化$\dfrac{1}{||\omega||}$和最小化$\dfrac{1}{2}||\omega||^2$是等价的，因此，问题进一步转化为<br>$$<br>\min_{\omega, b}\quad\dfrac{1}{2}||\omega||^2\tag{6}\\<br>s.t.\quad y_i(\omega\cdot x_i + b) \ge 1,\qquad i = 1,2,\cdots,N<br>$$<br>问题(6)就是支持向量机需要解决的问题，即原始最优化问题，最终我们求得的分类决策函数是<br>$$<br>f(x) = \text{sign}(\omega^\ast\cdot x + b^\ast)\tag{7}<br>$$</p>
<h3 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h3><p>对上述问题(6)建立拉格朗日函数，引入拉格朗日乘子$\alpha_i \ge 0, \quad i = 1,2,\cdots,N$，定义拉格朗日函数为<br>$$<br>L(\omega,b,\alpha) = \dfrac{1}{2}||\omega||^2 - \sum_{i=1}^N\alpha_iy_i(\omega\cdot x_i + b) + \sum_{i = 1}^N\alpha_i\tag{8}<br>$$<br>分别对$\omega$和$b$求偏导令其为零然后带入到拉格朗日函数(8)中去，即得到其对偶问题(关于对偶的相关知识不在此陈述)<br>$$<br>\min_{\alpha}\quad\dfrac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i = 1}^N\alpha_i\tag{9}\\<br>s.t.\quad \sum_{i = 1}^N\alpha_iy_i = 0\\<br>\alpha_i \ge 0, \quad i = 1,2,\cdots, N<br>$$<br>然而实际情况中，线性可分的情况是十分少的，一般都是近似线性可分的的，这里涉及到的是软间隔支持向量机，也称为线性支持向量机，是最基本的支持向量机。</p>
<p>对与近似线性可分的样本中的噪声样本通过引入一个松弛变量$\xi_i$，从而使其在一定的可接受的误差范围内可分，因此得到线性支持向量机的原始最优化问题<br>$$<br>\min_{\omega, b}\quad\dfrac{1}{2}||\omega||^2+C\sum_{i = 1}^N\xi_i\tag{10}\\<br>s.t.\quad y_i(\omega\cdot x_i + b) \ge 1 - \xi_i,\qquad i = 1,2,\cdots,N\\<br>\xi_i \ge 0, \quad i = 1,2,\cdots,N<br>$$<br>其对偶问题为<br>$$<br>\min_{\alpha}\quad\dfrac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i = 1}^N\alpha_i\tag{11}\\<br>s.t.\quad \sum_{i = 1}^N\alpha_iy_i = 0\\<br>0 \le\alpha_i \le C, \quad i = 1,2,\cdots, N<br>$$<br>可一看到，他和线性可分支持向量机的唯一区别是$\alpha_i$的约束多了一个上限。</p>
<h3 id="非线性支持向量机与核技巧"><a href="#非线性支持向量机与核技巧" class="headerlink" title="非线性支持向量机与核技巧"></a>非线性支持向量机与核技巧</h3><p>对于输入空间中的分线性分类问题，可以通过非线性变换将它转换为某个高维特征空间中的线性分类问题，在高维空间中学习线性支持向量机。</p>
<p>输入空间到某个高维空间的转换是通过映射$\phi(x):\chi \to H$来完成的，对于$\forall x,z \in \chi$，函数$K(x,z)$满足条件$K(x,z) = \phi(x)\cdot\phi(z)$，则称函数$K(x,z)$为核函数。</p>
<p>目标函数与分类决策函数中都只涉及实例与实例之间的内积，因此并不需要显示的指定非线性变换，而是直接用核函数来代替内积，对偶问题则变为如下形式<br>$$<br>W(\alpha) = \dfrac{1}{2}\sum_{i = 1}^N\sum_{j = 1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum_{i = 1}^N\alpha_i\tag{12}<br>$$<br>分类决策函数中的内积也用核函数来表示，则<br>$$<br>f(x) = \text{sign}(\sum_{i = 1}^{N_s}\alpha_i^{\ast}y_i\phi(x_i)\cdot\phi(x)+b^\ast) = \text{sign}(\sum_{i = 1}^{N_s}\alpha_i^\ast y_iK(x_i,x)+b^\ast)\tag{13}<br>$$<br>因此在线性支持向量机学习的对偶问题中使用核函数来代替内积即可解得非线性支持向量机。</p>
<h3 id="序列最小最优化算法（SMO）"><a href="#序列最小最优化算法（SMO）" class="headerlink" title="序列最小最优化算法（SMO）"></a>序列最小最优化算法（SMO）</h3><p>该算法分为两个部分分别是</p>
<blockquote>
<p>求解两个变量二次规划的解析方法</p>
<p>选择变量的启发式方法</p>
</blockquote>
<h4 id="两个变量的二次规划解析方法"><a href="#两个变量的二次规划解析方法" class="headerlink" title="两个变量的二次规划解析方法"></a>两个变量的二次规划解析方法</h4><p>假定选择两个变量$\alpha_1$和$\alpha_2$，而其它的变量$\alpha_i(i = 3,4,\cdots,N)$保持不变，则(12)式的子问题可以表示为<br>$$<br>\min_{\alpha_1,\alpha_2}\quad W(\alpha_1,\alpha_2) = \dfrac{1}{2}K_{11}\alpha_1^2+\dfrac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2 \\ \quad\quad\quad- (\alpha_1+\alpha_2) + y_1\alpha_1\sum_{i = 3}^{N}y_i\alpha_iK_{i1} + y_2\alpha_2\sum_{i = 3}^Ny_i\alpha_iK_{i2}\tag{14}\\s.t.\quad \alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta\\0 \le \alpha\le C,\quad i = 1,2<br>$$<br>将问题(14)中的等式约束$\alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta$进行变换，用$\alpha_2$来表示$\alpha_1$，将其代入到(14)式中，那么问题就变为关于变量$\alpha_2$的二次函数求极值的问题，求得$\alpha_2$之后通过等式$\alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta$即可得到$\alpha_1$的值。</p>
<h4 id="启发式变量选择方法"><a href="#启发式变量选择方法" class="headerlink" title="启发式变量选择方法"></a>启发式变量选择方法</h4><p>SMO称第一个变量的选择为外层循环，外层循环在训练样本中选取<code>违反KKT条件最严重</code>的样本点，并将其对应的变量作为第一个变量，具体的检验训练样本$(x_i, y_i)$是否满足KKT条件，即<br>$$<br>\alpha_i = 0 \Leftrightarrow y_i g(x_i) \ge 1\\ 0 \lt \alpha_i \lt C \Leftrightarrow y_ig(x_i) = 1\\ \alpha_i = C \Leftrightarrow y_ig(x_i) \le 1<br>$$<br>其中$g(x_i) = \sum_{j = 1}^N\alpha_jy_jK(x_i,x_j) + b$，改检验是在$\varepsilon$范围内进行的。</p>
<p>第二个变量的选择称为内层循环，在找到第一个变量$\alpha_1$的条件下，在内层循环中寻找第二个变量$\alpha_2$，第二个变量的选择标准是希望能够使$\alpha_2$尽量有足够大的变化。</p>
<p>关于SMO算法中的变量选择方法我目前并不是完全懂，先写到这里为止。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>此次总结对之前学的支持向量机的相关知识进行了一次回顾，支持向量机结合核技巧是一个非常好的学习算法，它利用了拉格朗日问题的对偶性，将原问题转换为对偶问题，因为可能存在一种情况是每个样本的属性非常的多，也就是输入空间的维数非常的大，而转换为对偶问题之后，参数是和样本一一对应的，因此参数数目就是样本的数目，这能够大大减小计算的复杂度，最后对于求解凸二次规划问题有十分有效的SMO算法，因为子问题有解析解，因此每次计算子问题都十分的快，虽然子问题数目多，但是总体上看来还是很快的。</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>大学生活回顾</title>
    <url>/20171226_NWPUReview/index.html</url>
    <content><![CDATA[<p>从来都不喜欢写作文之类的东西，主要是因为文笔不好，因此对于这方面没有兴趣，经常希望能有机会对自己过去的一段时间所做的和所经历的事情做一些回顾和总结。转眼就2018年了，前两天大家还都在考研，虽然我不考，但是我也有些小兴奋。。。最近一个月左右也过的很无聊，除了准备一下毕设的一些事情之外似乎也无所事事，今天下午同样是无聊，因此借此机会对过去的大学四年简单的总结一下。</p>
<a id="more"></a>

<h2 id="学习方面"><a href="#学习方面" class="headerlink" title="学习方面"></a>学习方面</h2><p> 在学习方面一句话就可以概括“这四年没有白混”。刚上大一的时候跟很多人一样，都对大学充满了好奇，东看看西瞅瞅唯一有些不爽的就是我有些路痴不识路，没有百度地图哪也去不了，近些天认识的一个朋友貌似对此也有些无语。因此当时我唯一希望记住的就是教室在哪儿，上课不能迟到；图书馆在哪，可以找个地方自习；过了好久才发现，原来我们学校也不大。我大一的时候基本上就是在教室图书馆宿舍食堂这几个地方待着，似乎看来除了学习很少干些其他事情，噢我是不玩游戏的很多人不信，最近闲下来才玩玩手游。由于是刚来的，多多少少还带有一些高中的特点，上课认真做笔记，老师一写一黑板我就一写好几页，现在还清晰的记得一件事，当时正在上郭老师的数学课，一个好像是督导的年纪有些大的人坐我旁边热情的跟我说不要以为来大学了就上了保险，还是要好好学习，我就一个劲的点头说嗯是的我知道，那堂课很愉快。在期中的时候，我们院自发的搞了一个期中考试，其实也就只考数学，让我们感受一下而已，当时我考了第二还是第一记不清了，不过有些高兴也有些骄傲，后来发现不是这样的，我以为就是数学英语这类课比较重要，其余的那些大家口中的水课我也是不太重视，以至于在大二大三提高总学分积上有些吃力。到大一下的时候有大四的分享保研经验时才知道有保研这回事，从此开启了学霸模式，大家喜欢这么叫，我其实并不怎么喜欢。从早上出去，基本上都是晚上10点左右回宿舍。在大二大三给我的感觉就是和我在高中没有什么区别，一学期十五六门课，作业都感觉没时间做，当然还有一些头疼的琐事，唯一的区别时没有人管你，你自己去安排自己的时间，我那时的状态可以借用一句话”沉迷于学习无法自拔“，当然这样必然会有些错过的东西，也许是一个亿，哈哈！也许是…其它。确实那一段时间并没有多少乐趣，过的有些枯燥，唯一有点点欣慰的是最后教务系统上出成绩了一看嗯还不错，总算有点收获，不然也许都坚持不下去。大三我们搬到市区了，但对于我来说没有什么区别，大唐西市就在隔壁，大三一年好像没去干过啥我总会说没人陪，这似乎成了一个借口。这段时间我唯一在乎的事情就是顺利拿到保研名额找到目标学校，各种准备各种关注，在今年暑假总算彻底完成了。虽然应该很高兴的，但是大四上学期我觉得过的并不是很充实，保研尘埃落定这段时间内似乎没有了学习的劲头，有空就睡懒觉看剧刷手机，大把的挥霍了时间，之前本来想的好好的保完研就做些有意思的东西，现在看来似乎是个笑话。总之大学里学习没有落下。</p>
<h2 id="生活方面"><a href="#生活方面" class="headerlink" title="生活方面"></a>生活方面</h2><p> 在大学四年里面除了学习只外当然还会接触到一些有趣的难忘的事情，我也是有好奇心的，刚来到大学我也报名了社团基地之类组织，但是那些什么乐队呀主持啊新闻啊之类的肯定跟我是无缘了，说白了我没有任何特长，普通得不能在普通了，我就加了微软俱乐部，但是我好像一次也没去过，那儿的人好像也一次也没找过我，我至今都不知道那里面是干啥的，有些吃惊。。。其他的一些什么搬桌子凳子我就更不可能去了，我好像没有那么的高尚无私，我可不想没有意义的在哪里搬东西收拾东西，目前作为预备党员似乎应该有点大公无私的精神哟！哈哈哈！我就欺骗一下我自己你们别当真！哈哈哈！还在ACM基地待过，但是由于水平不够，我又不想天天对着电脑成百上千的刷着机试题所以最后也没去。大二时我又报了一个机器人基地，最后也是由于类似原因半路退出，现在想想我好像不太能坚持看不到希望的事情，我原本是希望能通过基地或一两个奖的，但是由于各种乱七八糟的事情又放弃了。我没有完整的参加过一个社团，坚持不下来，但是感觉有些东西即使坚持了也许还是得不到，最近的一件事也是有些苦恼不知如何是好。在大一的时候班级聚餐或其他活动我也会去参加，跟大家一块玩，但是越往后去的越少，甚至都没有活动了。大学的班级不像高中，比较松散各玩各的。前两年都沉迷于学习，偶尔跟同学去几个景点逛逛，拍两张照片就很高兴了。只是这一学期倒还有些乐趣，和同学看了不少电影，也看了一两本书，以前是几乎不看的，所以最近总说原来这才是大学生活。。。在大学里面加了不少的好友，认识的不认识的，同班的不同班的都有，几乎都是男生，女生并不多，甚至我们班的女生都没有全加上。加的好友虽然多，但是常联系经常交流的并不多甚至说就那么几个，跟高中一样还在联系的就那么几个，一个手掌都能数完，一想到这难免有些难过。偶尔跟同学出去吃个饭聊聊天感觉还是很好的。昨天刚跟一个考完研的同学出去吃了饭，顺便看了个电影《芳华》没想到那么伤感。。。有一些感触，最近的状态不太适合看这种情感过于细腻的电影。我好像跑题了。。。一个朋友说我说话有点自娱自乐，好像很对呀哈哈哈！总之大学里面没完整参加过社团,没有到处闲逛，偶尔跟同学一起出去玩玩，感觉过得很平淡。</p>
<h2 id="有些遗憾的"><a href="#有些遗憾的" class="headerlink" title="有些遗憾的"></a>有些遗憾的</h2><p> 刚说了在大学里面虽然认识了很多人，但是那种经常联系一起吃饭聊天的不多，这也许是大学过的很枯燥很平淡的原因。机会只有一次，如果再给我一个机会结果可能还是会这样哈哈哈<del>~</del>有时有人问我有女朋友没有，我说我没有谈过恋爱哪来女朋友，有些人感觉很吃惊。我感觉很正常啊哈哈哈。。。再说也不容易遇到喜欢的人（不是眼光高哟）认识的女生也不多。偶尔会听到一个说法是大学就该谈恋爱，我也不知道是不是，哈哈不说了尴尬。还有我本打算在大学好好锻炼身体的，但是自己似乎总是有理由有借口，宁可坐着发呆睡懒觉也不想动，虽然我知道不好，但就是不想动不知道为什么，这是我最后悔的一件事。感觉大学里就没把学习落下，其余的什么事都没有做过。这大学几年看来过的并不完美。</p>
<p> 不管怎样，已经过去的无法在改变了，目前能做的就是最好未来的规划，也许还会同样的不完美，能做出一些改变也是好的。最近过的太悠闲，生活很没规律无所事事，也有一些事情比较纠结，总喜欢胡思乱想，希望能在元旦回去之后调整一下自己，希望在研究生阶段能弥补一些大学的遗憾，最起码要锻炼身体，其余的事再说，感觉我现在就开始讨价还价了。。。这样好像不好，希望不要给自己打脸，我看好我。最近有些颓废，马上就2018年了又老了一岁，希望自己写的这个垃圾文章能够警醒一下自己，革命尚未成功，仍需努力。fighting！！！</p>
]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>保研夏令营经验总结</title>
    <url>/20190428_SummerCamp/index.html</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>从关注夏令营到夏令营结束，花了好长的时间，这学期的课也没有选多少就是为了准备夏令营，参加了四个夏令营，“”旅游“将近二十天，天天在外面跑晒黑了不少（今年气温贼高，大多40度以上）。7月4号到7月6号参加北京大学前沿交叉学科研究院（以下称“叉院”）的夏令营，7月7号都7月9号参加清华大学计算机学院的夏令营，7月13号到7月16号参加南京大学计算机学院夏令营，最后一个7月17号到7月23号参加中国科学技术大学计算机学院夏令营。最终中科大offer确定！</p>
<a id="more"></a>

<p>我的个人基本情况：</p>
<blockquote>
<p>专业：软件工程<br>学分绩排名：8/235<br>英语四六级：526，490<br>数模美赛一等，数学竞赛全国二等和省一等，节能减排全国三等，还有一些校级一二三等奖</p>
</blockquote>
<h2 id="关注夏令营"><a href="#关注夏令营" class="headerlink" title="关注夏令营"></a>关注夏令营</h2><p>对于关注夏令营的问题，我觉得宜早不宜迟，越早心里越有准备。之前听说过不知道哪个学校的一个学姐到九月份了都还不知道有保研夏令营这么一回事^~^，最后还是走的九月正式推免这条路，还是早点了解为好，建议在大三下学期开始就着手。<br>这里有一些有用的网站以及app：</p>
<ul>
<li>保研论坛：<a href="http://www.eeban.com/" target="_blank" rel="noopener">www.eeban.com</a></li>
<li>导师信息网：<a href="https://www.mysupervisor.org/" target="_blank" rel="noopener">www.mysupervisor.org</a></li>
<li>保研通APP：<a href="http://www.973.com/xiazai/60337" target="_blank" rel="noopener">www.973.com/xiazai/60337</a></li>
<li>保研论坛APP：<a href="https://www.7down.com/soft/175489.html" target="_blank" rel="noopener">www.7down.com/soft/175489.h…</a></li>
</ul>
<h2 id="准备材料-amp-联系导师"><a href="#准备材料-amp-联系导师" class="headerlink" title="准备材料&amp;联系导师"></a>准备材料&amp;联系导师</h2><p>最重要的是准备成绩单和排名证明（前五学期），当时排名刚出来我就拉上隔壁班一个喜欢扯淡的邱同学一起去教务处打印成绩单和排名证明，我们学院教务处还好，不怼人，也不会以各种理由要留住你而不给你打印，我们貌似是最早的，还是那句话宜早不宜迟。打印出来之后最好拿复印店去彩印几份。<br>最麻烦也是最尴尬的一件事就是去找各个老师给你在推荐信上签字，推荐信自己电子版写好打印出来，然后找老师签字，因为我们学院的教授副教授就那么几个还是算上院长副院长，而要报的学校又那么多，么个学校大概在两到三封，多次找同一个老师会非常尴尬，邱某就是找了老师签了好几次，最后写错了一个地方又去找老师签字，老师当场怼他：”你到底要报多少个呀“。我也是，我去找院长签字的时候说“我记得给你签过，你报了多少啊，别到时报了又不去坏了西工大名声”，真是非常尴尬。建议最好是将所有的推荐信都准备好，需要某一个老师签的所有信一次拿过去给他让他签了，然后再去找其他老师。<br>  除了上述材料麻烦之外，剩余的就简单了，就剩个人简历，个人陈述，获奖证书复印件等，差不多就这些了，之后就开始联系导师了，我大概是在4，5月份开始联系老师，联系的方式是电子邮件，主页中写上自己的一些情况，例如教育背景，竞赛和项目经历，感兴趣方向以及未来规划等，最后在附件中附上简历，成绩单和排名证明足矣！我联系了很多，现在回去看看邮箱，来往信件共100多封。发邮件要广发你的目标学校和感兴趣方向的老师，但是一个实验室和课题的老师最多只发一个，否则有可能这个实验室或课题组的老师都不会要你。但是如果老师3到5天还没回一般是对你不感兴趣，你可以再次发邮件询问，或者换老师。</p>
<h2 id="报名-amp-等待入营通知"><a href="#报名-amp-等待入营通知" class="headerlink" title="报名&amp;等待入营通知"></a>报名&amp;等待入营通知</h2><p>我当时报名时的原则是”广撒网“，当然是要比本校更好的学校，不然保出去干啥。我报了好几个学校，有的入营了，有的被刷了，有的争取到机会入营：</p>
<ul>
<li>复旦大学&amp;中国人民大学：报名报了一半放弃了，因为了解到这两所学校在工科方面不是那么出众</li>
<li>北京航空航天大学：（吐槽）北航没有网上报名系统，只是通知邮寄材料，我用EMS邮寄过去，最后收寄结果显示的是一个不知道的地方收的，我打电话给快递员他说这个地方就是向北航送快递的，我以为是的，最后入营结果出来，我和邱同学都没有录取，而是好几个（并不是说他们不好）排名在我们后面并且竞赛也比一定比我们多的同学入营了，邱同学打电话过去问，那边说是核查之后没有收我们材料的记录，也就是我们的材料没有寄过去，那边还说”焉知非福，九月在来吧”。（fuck我不知道是EMS不靠谱还是北航把这不当回事）瞬间对北航没有什么意愿了，不去拉倒。</li>
<li>清华大学：准备好材料之后就寄过去了，网上也能看到状态，比北航好多了，最后入营结果快6月底才出，有点慢，但是最后看到名单里有我，还是非常高兴的，毕竟我们专业只有两个人进了。100多人入营，比例大概15%左右。</li>
<li>北大叉院：在等到入营结果出来之后，我没有收到邮件，也就是被刷了，但是我又打电话去问那边说已经确定了，如果有补录会考虑我，最后真的有人学校考试冲突去不了有补录，然后我就去了。机会都是争取来的，给学校那边打电话多问问。这是在刷了之后还有的最后一线希望。</li>
<li>上海交通大学：感觉上交有点傲娇，入营感觉很有难度，我们专业只有第一名入营了（他最后没去），而且听说给外校的学术型硕士名额极少，基本只要直博生，我们专业好像好几个确定去直博。</li>
<li>南京大学：网上报名之后，一段时间之后发邮件通知说通过筛选了，需要网上确认入营然后邮寄材料。</li>
<li>北大信科：结果出来的最晚的一个，最后被刷了，听人说是那边看到清华名的单了入营清华的都不要（不全是，除非你特别厉害）我们专业只有第五名的同学入营。</li>
<li>中国科学技术大学：不需要邮寄材料，全部扫描成电子版在网上提交，最后我们专业就我入营，好像是1200多人中筛选了120左右的人，大概10%的入营比例。</li>
</ul>
<h2 id="夏令营汇总"><a href="#夏令营汇总" class="headerlink" title="夏令营汇总"></a>夏令营汇总</h2><p>夏令营从7月3号正式开始了。</p>
<h3 id="北大叉院夏令营"><a href="#北大叉院夏令营" class="headerlink" title="北大叉院夏令营"></a>北大叉院夏令营</h3><p>北大叉院夏令营开始最早，7月3号晚上坐火车去北京（第一次去北京），第二天大概早上8点多就到了，在火车上的时候还和之前联系的一个叉院老师邮件沟通了一下，他让我加他微信方便联系，感觉是个好的开始。从火车站坐公交到北大之后先去报道，教务处说下午两点再来。目前时间大概在10点左右，那个老师又打电话来了，问我到了没有，现在有没有时间去和他见个面，我来的比较匆忙，现在肯定不是最好的时候，我就说我刚到火车站，要不下午去见面，然后就定在了下午一点。我赶紧先到学校外面找了个饭馆吃了个饭（不知道算早饭还是午饭），然后在北大的校园里一个角落里坐了一会儿，那天真是热呀，真受不了，还好我电脑包里面有一个把扇子。大概12点40左右就去老师门口等着，他还没来，期间他的一个学生看我在等老师，他就打电话给老师说我到了，一会儿老师就来了，我们在他办公室坐下，我们面对面坐着，简单的客套了一下就转入正题了，他说问我几个问题，他问我当x很小的时候sinx大于什么东西，我印象中是在x趋于0时sinx &gt; x,我就回答了x，然后他看我回答挺快的就问为什么，说实话这个问题换做谁闭着眼睛都能答出来，我当时可能紧张了竟然忘了，扯到了x趋于0是x/sinx的极限是1，他问然后了，我知道不对，但是真的突然忘了，我就赶紧换其他方法，我说用几何方法吧，单位圆来证明，确实解决了。老师说嗯问题确实解决了，但这不是我想要的答案，我本来是想要你用微分的角度来解决的，不过你能想其他办法解决这个问题还可以吧。（结束之后想到直接是x-sinx然后求导证明单调就完事了，当时真的突然记不起来）然后下一个问题问我一个统计的问题，说如何从统计的角度求解pi的值，这个问题太简单了，高中生都会，我很快就答出来了，然后让说你之前是不是接触过这个问题，回答这么快。（不应该回答这么快，让老师觉得你接触过这个问题，现在你即使答上来也看不出你的水平，应该装着思考一下）。然后说我在问你一个问题吧，还是统计方面的，说如何知道100个人的平均分数，而且又不想把每个人的分数看一遍，我想了一会儿说随机抽样吧，他嗯了一下表情和奇怪，我不知道对还是错，他也没说对错（后来问别人，都说是抽样，我现在都不知道对还是错，他表情真的很奇怪），然后他就这个问题延伸又问我一个问你，我没听明白，他又说了半天我还是没听明白，尴尬了，我问题都没听懂咋回答。就一直在那跟他扯，他估计认为我不会就说了答案然后还给我解释，到现在我还是不明白他要问什么！！！然后他说我在问你一个问题吧，说这是他当时读博士的时候他导师面试他的问题，现在问问我，说给你一个半正定对称矩阵<strong>A</strong>，求矩阵<strong>A</strong>的n次方，或者换句话说在n很大的时候会发生什么，说实话这个问题我当时真不会，想了好久，他说给我提示就是把矩阵<strong>A</strong>分解，然后再求解，我说三角分解，表情一愣（感觉不对）说然后呢，我说不下去，我也不知道了，感觉越分解越乱啊，我们就只学了三角分解，然后我有想了半天，他知道我不会就说是矩阵的奇异值分解，我一愣说我不知道这个呀，我们就没学过这个分解，答案是很简单，但是我根本就不知道这个分解呀，可能是线性代数没学好吧。。。然后又客套了一下就结束了。哎，不会就是不会，他才不管你学没学过，平时还是应该多积累，多深入，不能只停留在老师讲的那些东西上。然后下去报道，领衣服，回宾馆休息。晚上去参加开营仪式，在一个大的报告厅，里面非常的豪华（无法形容），开营仪式上各个方向都有代表老师上来做介绍，大概半个小时到一个小时的时间就结束了，第二天就是听各个老师的报告会，各个老师上去把自己的方向吹一吹来吸引学生，下午大概3点左右有笔试，我们方向是笔试和机试可以选机试和笔试或者只选机试，机试必选，统计学方向是笔试必选，机试自愿。笔试这段时间我回去准备晚上的机试了，晚上的机试一共10道题，只要平时认真写过代码的做3道题以上没问题。我当时做了5题，然后第六和第七题一个是最短路路径问题一个是深度搜索，这两个我都能做出来，我先做的最短路问题，代码写完了但是这题提交就是通过不了，英文题目读了好几遍觉得没问题但就是过不了，结果这最后的一小时都耗在这题上，最后这两题都没做出来，后来发现是一个英文单词理解错了导致题意弄错了，其实只要代码稍微修改一下就可以了，但是题目意思弄错了真的没办法，英语水平还得好好提高。看到实时排名我做了5道题排25名，共七八十人吧，还能接受吧，我认识的我们学校的和我在内一共三个人，我做的最多，一个4题还有一个3题。机试完感觉一般吧，回去好好睡觉第三天准备面试。晚上的时候面试安排出来了，我在下一点左右，其实我们那组面试很快，老师们下午就在原地吃饭，吃完第二个就到我了，我进去之后老师先让我自我介绍，我忘记有没有要求英文，但我自我介绍是说的英文（准备好的），完了老师就开始看着材料问问题，简单说了一下竞赛，项目，我有一个软件测试竞赛省特等奖，问我为甚能得特等奖，能不能拿来创业，我就说我做的还不错，把学的知识运用的比较好而已，创业还达不到这个水平，然后就没有然后了。共四个老师，中间靠左的那个问我一个复变函数的问题，后来又有一个老师问我信号与系统的问题（因为看我成绩单上有这门课），之前我都是准备的机器学习方面的东西，比如线性代数，概率统计，数学分析，机器学习，谁会去复习信号与系统啊，我们专业选机器学习方向有谁回去看那本600多页的大厚书，的有我一直在凭着印象来说，结果肯定是不理想，然后就结束了，感觉整个过程都很不爽，没有一个问题和我准备的沾边的，后来了解了一下别人的情况，不都是这种问题，看来还得有一定的运气，充分的准备和一定的好运气才有好结果。然后就是回去等结果，收到邮件就录取了，没收到就是被刷了，很遗憾我被刷了。我们学院的第一名和我两个人都没有录取。</p>
<h3 id="清华计算机夏令营"><a href="#清华计算机夏令营" class="headerlink" title="清华计算机夏令营"></a>清华计算机夏令营</h3><p>7月7号到清华大学去报道之后再宿舍休息了一会儿，晚上就直接是机试，这次机试一共三道题，三道题基本上都是模拟类型的题目，第一题是给你一个矩阵然后输入不同指令对矩阵进行反转等各种操作，第二题是让你输入一段代码然后将代码中注释去掉然后再输出，第三题记不太清了。我做完了第一道和第三道，第二道不太会（字符串处理是硬伤）。这两道题通过自定义测试应该是没问题的，但是去清华的好多都是ACM大牛，做不出三道基本没戏，清华的一个政策是夏令营是没有面试的，不录取任何人的，机试前50名九月份直接获得面试资格，没进前50的如果能够找到老师九月推荐你面试你也可以直接获得资格，每个老师最多推荐三个，也就是说只有上面两种方法你能9月份去面试，否则你再次报名会和九月报名的人一起筛选，不一定会轮得到你，所以夏令营做好机试或者找好老师推荐很重要。我开始还以为机试做的不错，可是牛的人很多，不全做出来基本没戏，然后我就去找之前联系的老师，问问他能不能推荐我九月份来面试，还好老师答应了。但是自己感觉没有前50名那么稳，毕竟人家的机试成绩在那里。只能说九月试试运气，不能抱太大希望。<br>第二天就是听各个老师的报告会，个人都来吹一下自己的方向多么多么好多么多么有前途。一整天都是，有点无聊。<br>第三天就是最后的师生交流，每个老师都在外面摆一个牌子，感兴趣的直接去找老师交流，了解情况，其实这个时间就是用来给你联系老师的，看有没有老师愿意推荐你，如果找到了老师愿意推荐，那么你九月份就还有一线希望，但不要过于乐观。了解完了之后在12点左右有一个冷餐会，吃点东西就可以走人了。</p>
<h3 id="南京大学计算机夏令营"><a href="#南京大学计算机夏令营" class="headerlink" title="南京大学计算机夏令营"></a>南京大学计算机夏令营</h3><p>清华夏令营结束后回学校休息了两三天，这几天正是高温，也没怎么休息好，7月13号晚上就坐上火车去南京，然后又做了好久的地铁才到，南京大学真是有点偏远。报完到之后就去宿舍休息了，这宾馆十分高档，应该是我住过最高档的。休息了一会儿一个我们学校的本专业同学发来消息问我在哪，问我现在去不去找老师，我想了一会儿就答应了，和他一块去找老师，准备提前联系一下，去了之后他说准备去找lamda（了解的人知道lamda有多牛）的老师，我想都没想过，我觉得有点难吧，我先去和我之前联系的老师见了一下（他不在，和他的博士聊了一会儿），然后有和那个同学去lamda实验室试试，lamda在5月份就有过一次面试，我当时没去，没想到现在听说还有几个名额，就在这下午就准备给大家面试一下，还好我来了，不然又错过机会了，但是来了也有将近10个左右，但是名额不超过4，5个，我们排好队面试。轮到我了，我进去没有自我介绍直接开问，问了几个数学问题，我之前准备的可以，回答这些没任何问题，老师们看起来很满意，然后我又把他们往我喜欢的机器学习等方向上引，让他们问这些方面的问题，因为我也准备的还可以，果然他们问了，结果可想而知，比较满意，一个老师当场就说你那个报名表带来没有，我现在给你签字吧（南大必须要有接受你的老师给你签字，然后再去通过学院的面试就可以了），尴尬，我没带进来，在门口排队时放在袋子里了，想着没必要带进去，没想到老师当场要给我签字，我说我出去拿，他说不要拿不要拿，那就明天早上你在拿来吧，我说好。其实他是不想让外面还在排队的人知道我被接收了，我其实也明白，因为老师们跟之前的同学的回复是你回去等消息吧，我们筛选之后再通知你们，所以我就答应明天早上再拿来签字。出来之后很高兴，没想到lamda这么轻松的就拿到名额了。感觉已经没有问题了（没想到学院面试会很坑），吃完饭高兴的回去了。<br>第二天就是开营仪式，然后就是各个实验室的摆出牌子来宣传，还可以去实验室参观，演示一些研究成果来吸引大家。因为lamda已经接收了，继续了解了一下之后就走了，毕竟已经签字了。<br>第三天就是学院的面试，我是第二个面试，南大不得不说做的很不好，我进去了，共三个老师，之后老师问有没有简历，我开始就没打算给简历的，因为南大在面试之前就说了啥都不用带，直接进去就行了，不让带简历的，我进去之后却张口找我要简历，这不是逗我玩了吗，我在想这老师昨晚喝多了吧！他们说要简历我不得不给要，可是在书包里面，还好我把书包带进去了，但是翻简历过程很尴尬呀，真不知道该咋吐槽。没有要求我做自我介绍又是直接开始问问题，我想也好，但是这个提问老师好像就是软件工程和软件测试的，他上来就直接问我什么是软件工程，只有被问过这个问题的人才了解我此时的心情，之前有个去北航面试的同学也被问过类似的，结果坑死了。我凭着印象简单说了一下，答案不是那么理想，然后第二个问题更加让你欲哭无泪，他问研究软件和研究软件工程有没有什么区别。。。我想郑老师（我一个老师）快来帮我解答一下吧。。。此时的心情没人能理解，我想了一会儿回答说有一定区别，刚说完“有区别”三个字立刻就被他打断了说其实没有多大区别，他扯了一堆理由解释，我在想你这明显就是故意的嘛，这个问题本身就是见仁见智的问题，说出自己的理解就可以了嘛，他却那么快打断我却发表自己的看法。一个旁边的老师看我比较尴尬就打圆场说这应该不是他想选的方向，那个老师也不知道咋说的记不清了，然后又接着说用英文解释一下什么事软件测试，必须英文，没办法憋了一两句解释了一下就不会说了。然后就结束了，感觉不太好。这个过程没问一个数学类或者研究类问题，我想对一个想做研究的人你净问这些虚的模能两可的问题。面试分为很多组，听说其他组都问的很正常很稳，我们这个组被很多人吐槽。说到底还是自己平时对一些常见的基本问题不够重视，计划赶不上变化，平时多积累多了解一点还是没错的。</p>
<h3 id="中科大计算机夏令营"><a href="#中科大计算机夏令营" class="headerlink" title="中科大计算机夏令营"></a>中科大计算机夏令营</h3><p>中科大夏令营是17号开始报道，因为之前联系的老师说提前见个面并且来个机试，所以16号南京大学的夏令营一结束下午立刻就坐上高铁去了中科大，大概5点多到的，有点晚，我去的时候老师已经走了，他的学生说让我晚上8点左右过来，我就先回宾馆了吃完饭休息了一会儿，大概7点40左右我就出发去学校找他，先是给我做一个机试，一共四道题，全是英文的，而且题目十分长废话很多，需要你快速读懂英文题目，总共就小时，不可能做完4题，所以我的计划是抓点紧做完两道就OK了，后来才知道其实做多少不重要，只要能做出一道就可以，因为中科大这四道题没有像其他学校那样给你几道送分题，从第一题开始就涉及到一点点算法。结束之后一个研究生把我叫去了另一个办公室，包括他里面共有三人，有一个是博士（后来了解到这个博士非常优秀），然后他们开始和我聊天（后来才知道其实就是面试，如果过了才会推荐给老师最终面谈一下），聊的范围比较广，从兴趣去爱好到个人的特点再问一些专业知识，还问一些感兴趣的研究方向上的问题，无非就是看看数学的基本功和对该研究方向的了解程度，之前就说过了，这方面我准备的比较充分，没有什么问题，整个过程很轻松。结束之后一看时间有点晚了，我一个人来这里不太熟让我赶紧回宾馆，还说让我找个时间去和老师见一下，也就是让老师最终在看看可不可以，这次聊天大概40到50分钟。结果就是比较满意。然后第二天就是正式报道举行开营仪式，中科大是所有专业同时举办夏令营的，所以好多人，开营仪式非常正式，整个夏令营的活动都非常多，我觉的中科大的夏令营才是真正意义上的夏令营，有学术交流，有休闲娱乐。下午就是师生见面会，副院长也就是我后来的导师做了开场报告，整体介绍了一下各个实验室，然后就是老师和同学们之间做游戏，副院长报告完就又走了不能和我们一起‘玩’，走之前有一个非常有意思的场景，我和我们学校计算机学院的一个女生遇到了，她也是想报这个老师的实验室，跟我说咱们赶紧去和他聊聊，他又要出去开会了，我就和他一起去了，我先和老师说了我是西工大的之前联系他的某某某，他知道我了，然后说好的，那个女生也说我是西工大的某某某，我也是报您的实验室的也说了一番，老师笑了一下，心想这么多西工大的，说我们实验室后面有机试好好准备啥的。我已经做过了，她还没有做。这是一个男生看我们说了这么多，他也出来说我也是西工大的某某某（也是计算机院的），现场师生都笑了，西工大的咋都找这个老师，别急还有，这时又来一个男生跟这个老师说我也是西工大的某某某，同样说了一翻，这场面真少见，真的有意思，看来西工大的想霸占这个实验室啊。。。这次我们学校来了4个，计算机学院1个女生两个男生，软件学院就我一个。老师走后所有学生和其他老师做游戏（有点意思）。在晚上有个别实验室有面试，我去了一个（这个是和我最后想去的实验室应该是是最好的两个实验室了）参加面试，没想到人好多，每个人都发两页英文的纸，上面是他们写过的论文的前两页，基本都不一样，需要你在这短时间内看懂，然后面试轮到你了就去，我看了一会儿，我就感觉我对这个方向不感兴趣，我大看了一下是讲什么的然后就没看了，到我面试了，我进去之后两个老师（人比较亲和）看了我的简历然后问了我一些问题，然后问我这两页纸看的咋样，我大概说了一下，他们就说可以了可以了，说这么短时间能看到这个程度很不错了，整个过程很愉快。然后我就会宿舍了。到宿舍休息了一会儿我就洗澡去了，出来一看手机有一个未接来电，我回了过去，他说是刚才面试的老师，说我面试通过了，如果愿意来这个实验室的话明天来签个双方确认书，我答应了。挂了电话后我很高兴，但是有些纠结。第二天，上午参观先研院和实验室，结束后我就抽空去昨天打电话的老师那里签字，他给我一张纸，我填了一般之后犹豫了，我又问了一下这个实验室的研究方向，我不太感兴趣，场面十分尴尬，老师又给我解释他们是干什么的，但是我依然不感兴趣，手里拿着那张表真不知道咋办，老师看出来了就说你还报了其他实验室吗，我如实说了，我之前联系了做大数据的实验室，我说我想做人工智能方面的，他说那个实验室是做人工智能出身的，但我们实验室实力也挺强的。（这些我都知道，这两个实验室是科大计算机实力最好的两个）我又说我想能不能我明天去那个实验室面试之后再做决定（此时我心里确实不太喜欢这个方向），当然老师也不想做备胎，就说嗯也可以，但是你现在不确认明天我们不能保证有名额给你，因为我们也有备选人，我们现在就提前决定给你名额是因为觉得你比较那啥（你们懂的），我手里拿着确认书，上面写着签了就必须来，所以我当时面临两个选择，一是直接签了确认，我的夏令营就结束了，后面学院面试不出什么大问题就OK了，第二个选择是放弃这个实验室，赌一把，赌我明天和之前联系的老师（副院长）见面比较成功然后去我感兴趣的方向，但风险就是，明天我见面不顺利，那么这两个最好的实验室都没戏，只能去找其他实验室了。我犹豫了好久，而且是在如此尴尬的情况下，当时我和老师都是站着的，最后我思考了很久，做出了决定，我决定赌一把，因为我读研肯定是要选我感兴趣的方向，不然我未来的硕士阶段将过的很不舒服，还不如直接出去工作，最坏的打算大不了夏令营失败了，九月份再战其他学校，我也说不准我的决定是对是错，我想不管对错，适合我的才是最好的，以后的路还长，所以我大胆的决定直接拒绝，我就跟老师说，那算了吧，我在考虑考虑吧，老师也看出来我的想法就说好吧，明天你要想来我们也欢迎但是不能保证给你留名额（我当然知道是套话，肯定没名额，还有那么多候选的），出来之后感觉轻松了许多，刚出门没多久就看到刚刚出去的那个同学又被叫回去了，我拦住问了一下，他是刚才老师让他回去等通知的同学，我一出来老师就给他打电话说实验室筛选好了让他过去签字，我心里很清楚，我一出来名额就让出去了，我的名额给他了。。。我都料到了，也做好了最坏的打算了，而且此时我并不为刚才的决定后悔。这天结束后，我们都回宿舍休息，第三天也就是19号户外活动，参观科大讯飞等地方，晚上我就去找老师见面了，因为他最近去外面开会了比较忙，还是晚上9点多一回办公室我就找的他，我把简历给他，他看了一下然后问了我一些问题，没问太多，了解了一些我的情况，因为我之前就聊过好多了专业问题，如果不行的话他的学生也不会把我推荐给他，最后就结束了，感觉还不错。晚上回去我就问之前微信加的一个学长（这个老师的博士）今晚的结果什么时候出来，他说明天就出来了。过了一会儿就睡了，第二天早上刚下车（科大都是用专车在宾馆和学校之间接送，科大这点做的比较好）之前面试我的博士生打电话给我说我面试通过了。挂完电话我顿时感觉心里的石头落地了，看来昨天博了一把那个选择是对的，然后这一整天就过的很轻松了，都忘记这一天干都是啥活动了，最后一天21号了，学校正式面试开始，时间是一上午，我排在中间，我等前面的等了好久，到我了我进去之后看到有三个老师，两个男老师一个女老师，一个老师让我说一下自己的特点，我巴拉巴拉说了一堆之后老师们对我很认可，两个男老师说嗯不错，很雷厉风行啊，这个特点要保持。然后就问了我一个简单的问题说无向图的深度搜索结果是否唯一，然后就结束，那个女老师就没吭过声，男老师问她还有没有想说的，她说可以了没有了。然后就结束了，感觉好快，瞬间就结束了，一个特点的介绍吸引住了老师后就特别顺利了。下午回去后面试中的两个男老师竟然都打电话过来问我，第一个老师说你联系好老师没有，我今天面试对你印象很好，有没有兴趣来我们这实验室，我当然会拒绝，因为我喜欢的那个实验室已经联系好了，我就委婉的回答，我就说我联系好老师了，他说让我等今天通知出来再决定要不要我（其实已经要我了，我是顺便试探一下我今天的面试过了没有），老师立刻说你已经通过了，你如果没联系好老师可以来我们这里，我说好的谢谢老师，我会考虑的，然后老师说你想来可以打我这个电话，谢谢老师之后就挂了。过了一会儿面试我的第二个老师也打电话来了（这个老师是做系统方向的，也是科大很牛的教授，跟他的一个本科毕业于科大的博士聊天时知道的）说今天非常看好我，问我有没有找好老师，想不想去他那里，我说我找好了，谢谢了老师之后就愉快的挂了电话。心情十分的高兴，没想到这么多老师希望我去他们那里（好多同学都没联系到），并且再次为那一次决定感到高兴，去了自己最想去的实验室。下午两三点实验室老师发来短信说要和我们筛选出来的学生一起见面，我去了后发现包括我在内共五同学，也就是说这次夏令营这个实验室只招了5个人，这次报这个实验室的大概有三四十人，最起码那天我们西工大的四个都找了这个老师的（再次感到那天很有趣），所以说我们还是很幸运的。老师和我们交流了一下，然后散了，吃完饭去参加闭营仪式，办的也十分好，10几个节目的表演，像晚会一样。次日早上也就是22号坐上火车返回西安。这次科大夏令营心情充满波折，过程非常愉快，最后的结果很满意。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>平时在学完课上东西之后还是需要再拓展一下，尤其是在自己感兴趣的方向上深入，否则别人会觉得你自己喜欢的东西都不了解那你还了解啥。</li>
<li>要注意多积累，主要是一些本专业本领域的一些常识性问题要重视，不管是不是你喜欢的专业，既然选了就要好好对待。</li>
<li>然后英语要不要丢掉，学了这么多年不能白学了，尤其是以后做研究搞学术还是需要的，面试的时候虽然不一定会问，但是如果问了就懵了，英语还是要坚持学。</li>
<li>还有就是在面临选择是要坚持自己的意愿，不能被眼前的利益所诱惑，该放弃的要果断放弃，适合自己兴趣的才是最好的。</li>
</ul>
]]></content>
      <tags>
        <tag>动态</tag>
      </tags>
  </entry>
  <entry>
    <title>参加CIKM会议</title>
    <url>/20191107_CIKM/index.html</url>
    <content><![CDATA[<p>第一次参加国际会议，初来乍到</p>
<p>太难了，紧张得不要不要的</p>
<p>报告前8小时，手机摔坏了</p>
<a id="more"></a>


<p><img src="https://img.chsong.live/CIKM-2019/IMG20191104082446.jpg?x-oss-process=style/m" alt="lG3sr8.jpg"></p>
<h2 id="Presentation"><a href="#Presentation" class="headerlink" title="Presentation"></a>Presentation</h2><p>主持人能不能别催，挺醒我: “one minute，one minute”</p>
<p>我: “OK” （此时实验才讲一半）</p>
<p>我: “since the limitation of the time, we skip the case study” （有点紧张随口说的，不知道有没有语法错误）</p>
<p>预先准备时正常读完备注需要8分钟，要在7分钟讲完，还得时不时看看大屏幕和观众以免僵硬，好难呀</p>
<p>还是ppt的内容放多了，之前没舍得删掉</p>
<p>那个提问的小姐姐，声音小了点啊，主持人真是的，也不给个话筒</p>
<p>我瞎扯一通不知道合不合你的问题</p>
<p><img src="https://img.chsong.live/CIKM-2019/lGBuIP.jpg?x-oss-process=style/m" alt="lGBuIP.jpg"></p>
<h2 id="Poster-Session"><a href="#Poster-Session" class="headerlink" title="Poster Session"></a>Poster Session</h2><p>开始我和左右两边的两个外国大兄弟，还没有人来找我们交流，我们就聊了聊，互相介绍工作</p>
<p>一个爱尔兰的，一个加拿大的</p>
<p>让我说还行，听是真的有点头疼，能知道你做的啥东西，专业词组听不懂，实在不知道细节</p>
<p>emmmm，可是还得保持微笑，还附和着：“Yeah, Yeah, Good”</p>
<p>叫Felix的大兄弟，走的时候给我来一句中文：“很高兴认识你”</p>
<p>我：”what? what?”</p>
<p>你可以直接说nice to meet you, 这我还是能听懂的</p>
<p><img src="https://img.chsong.live/CIKM-2019/lGB956.jpg?x-oss-process=style/m" alt="lGB956.jpg"></p>
<h2 id="Banquet"><a href="#Banquet" class="headerlink" title="Banquet"></a>Banquet</h2><p>小吃有糖葫芦，糖人，吹糖人</p>
<p>糖葫芦味道挺棒虽然有点酸</p>
<p>师兄们不要笑，糖葫芦挺好吃的，咋不来尝尝呢</p>
<p>还有武术，舞狮，戏剧，大鱼海棠，满满的中国风</p>
<p>从提出idea到今天，整个过程难呀，感谢师兄师姐，还有导师们的帮助</p>
]]></content>
      <tags>
        <tag>动态</tag>
      </tags>
  </entry>
  <entry>
    <title>牛客-商品交易</title>
    <url>/20200710_%E5%95%86%E5%93%81%E4%BA%A4%E6%98%93/index.html</url>
    <content><![CDATA[<p>这题在牛客网上标签有<strong>动态规划</strong>和<strong>贪心</strong>，因此，用可以用动态规划和贪心两种算法来解决。</p>
<a id="more"></a>

<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>珐达采下个月要去鸥洲各国考察一趟，采购流通神秘石并从中搞点油水。<br>珐达采会按顺序依次经过序号分别为$1, 2, 3, …, n$的鸥洲国家，在第i个国家神秘石的流通价格为$A_i$鸥。因为行程紧张，在每个国家的停留时间有限，所以他只能花费$A_i$鸥买入一块神秘石，或者卖出一块手中的神秘石获得$A_i$鸥，或者什么都不做，而且因为神秘石的保存需要极其先进的高级材料容器，其材料稀有且制作困难，珐达采只有一份容器，故无论何时珐达采手里 最多只能拥有一块神秘石。<br>珐达采想知道最终能从中获利最大多少鸥。因为交易需要手续费，所以珐达采还想知道在获利最大收益的同时，最少需要交易多少次。因为珐达采是大财阀，所以你可以认为他一开始金钱无限。</p>
<h2 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述"></a>输入描述</h2><blockquote>
<p>第一行一个数$n$。$1\le n \le 100000$</p>
<p>第二行n$n$数，第$i$个数表示$A_i$。$1\le A_i \le 1e9$</p>
</blockquote>
<h2 id="输出描述"><a href="#输出描述" class="headerlink" title="输出描述"></a>输出描述</h2><blockquote>
<p>共一行，两个数，分别代表最大收益和对应的最少交易次数。</p>
</blockquote>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>s输入</p>
<blockquote>
<p>5<br>9 7 10 1 5</p>
</blockquote>
<p>输出</p>
<blockquote>
<p>7 4</p>
</blockquote>
<h2 id="方法分析"><a href="#方法分析" class="headerlink" title="方法分析"></a>方法分析</h2><p>初步分析后，此题的意思就是，给定$N$个数，按顺序依次经过这$N$个数，当处于第$i$个数的时候，可以以第$i$个数为价格买进或者卖出宝石，有一个限制条件就是手里最多只能买进1个宝石。</p>
<h3 id="暴力穷举"><a href="#暴力穷举" class="headerlink" title="暴力穷举"></a>暴力穷举</h3><p>那么每经过一个数的时候，有且只有三种操作：1-买宝石，2-卖宝石，3-不买也不卖。定义从第一个数到最后一个数，对每一个数所作的操作构成的序列称为一个操作序列。因此，给定N个数，最多有$3^N$个操作序列，当然其中肯定有不合理的序列（简便起见，不作过多解释），那么用暴力穷举的算法，时间复杂度接近于$O(3^N)$。</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>动态规划算法最核心的要素有两点：<strong>状态；状态转移方程；</strong><br>状态选取的好坏有时候可以决定这个问题能否被解决。一般状态的选取要满足两个条件：<br><strong>1. 最优子结构</strong><br><strong>子结构</strong>：子结构指的是，一个问题可以分解为多个多个子问题，每一个子问题就称为一个子结构，而子问题又能分解为更小的子问题，形成更小的子结构。<br><strong>最优子结构</strong>：每一个问题或子问题的最优值，都能够有各自的子问题的最优解推导而来。而每一个子问题的最优解即为<strong>状态</strong>。<br><strong>2. 无后效性</strong><br><strong>无后效性</strong>：无后效性指的是，我们只关心每一个子问题的最优值<strong>是多少</strong>，而不关心这个最优值是<strong>怎么来的</strong>。换句话说就是，无论当前这个子问题的最优值是采用什么方法怎么得到的，都不影响后续问题（当前子问题的父问题）最优值的求解。</p>
<p>对于本题，给定$n$个数$[a_1,a_2, \dots, a_{n-1}, a_n]$目标是求顺序经过这$n$个数后的最大收益。子问题可以定义成：求顺序经过$[a_1, a_2, \dots, a_{n-1}]$这$n-1$个数的最大收益，然后再决定在第$n$个数的时候采取哪种操作，是<strong>买</strong>、<strong>卖</strong>还是<strong>不买也不卖</strong>来判断原问题的最优值。</p>
<p>首先，这里我们定义子问题【求顺序经过$[a_1,a_2, \dots, a_{i}]$这$i$个数的最大收益】的解也就是状态，为<strong>dp[i]</strong>。</p>
<p>然后，在第$i+1$个数的时候，判断采取哪种操作，得到子问题【求顺序经过$[a_1,a_2, \dots, a_{i+1}]$这$i+1$个数的最大收益】的最优值<strong>dp[i+1]</strong>。<br>这样似乎很好，可是在知道了<strong>dp[i]</strong>之后，却不知道经过$i$个数后手里有没有宝石，那么在判断第$i+1$个数采取哪种操作的时候就没有判断依据，因为都不知道到了手里还有没有宝石。<br>那么我们可以引入一个参数<strong>own</strong>(取值True或False)指示手里有没有宝石，如果宝石被卖掉则own置为False，如果买进则置为True，否则不改变own的值。</p>
<p>以上过程似乎很合理，可是当我们在决策第$i+1$个数的时候，我们只能知道own是True或者False其中一种情况。换句话说，如果此时own为False，那我们就知道第$i$个数决策后手里没有宝石，此时我们会在第$i+1$个数上决策到底买还是不买。可是我们不知道如果own为True，也就是第$i$个数决策后手里有宝石的情况。你可能会说，既然own为False了，那么肯定own为False是第$i$个数的最佳决策结果，很正确。可是存在一种情况就是，我在第$i$个数的时候拥有宝石，也就是own为True（买进或者不卖）虽然比不拥有宝石的收益小，但有可能我在后面卖掉获得的收益会超过当前own为False的收益。</p>
<p>所以我们要对比有或没有宝石两种情况，在任何一个中间数$i$上，我们要求出有宝石的最大收益和没有宝石的最大收益。两者虽然不等，但后面有可能有宝石的超越没宝石的，或者相反。最终的结果要到最后一个数的时候才知道。因此我们要同时记录有宝石和没宝石两种情况。因此我们将<strong>d[i]</strong>扩展为<strong>d[0][i]</strong>和<strong>d[1][i]</strong>，前者表示在第$i$个数决策完之后手上没有宝石的收益，后者表示在第$i$个数决策完之后手上有宝石的收益。</p>
<p>对于<strong>d[0][i]</strong>，在第$i$个数决策完之后手上没有宝石，有两种情况。一种是在第$i-1$决策完之后手上本来就没有宝石，那么为了保持在第$i$个时刻没宝石，就不能买进，所以<strong>d[0][i]</strong>的收益就是<strong>d[0][i-1]</strong>；另一种是在第$i-1$决策完之后手上有宝石，那么为了保持在第$i$个时刻没宝石，就必须在第$i$个时刻卖掉，所以<strong>d[0][i]</strong>的收益就是<strong>d[1][i-1]</strong> + a[i]。这里的a[i]是宝石在第$i$个数上的价格。为了使得<strong>d[0][i]</strong>最大，所以求一下两者之间的最大值：<br>$$<br>d[0][i] = max(d[0][i-1], d[1][i-1] + a[i])<br>$$</p>
<p>对于<strong>d[1][i]</strong>，在第$i$个数决策完之后手上有宝石，也有两种情况。一种是$i-1$时刻没有宝石，另一种是有宝石。分析同上，试着分析一下。最后可以得到：<br>$$<br>d[1][i] = max(d[1][i-1], d[0][i-1] - a[i])<br>$$<br>第二个公式为什么是减去a[i]？仔细想一下。</p>
<p>到此为止，我们即可得到动态规划要求的每一个时刻的状态d[0][i]和d[1][i]，以及状态转移方程：<br>$$<br>d[0][i] = max(d[0][i-1], d[1][i-1] + a[i])\\<br>d[1][i] = max(d[1][i-1], d[0][i-1] - a[i])<br>$$</p>
<p>根据状态转移方程即可得到动态规划核心代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">	d[<span class="number">0</span>][i] = max(d[<span class="number">0</span>][i<span class="number">-1</span>], d[<span class="number">1</span>][i<span class="number">-1</span>] + a[i])</span><br><span class="line">	d[<span class="number">1</span>][i] = max(d[<span class="number">1</span>][i<span class="number">-1</span>], d[<span class="number">0</span>][i<span class="number">-1</span>] - a[i])</span><br><span class="line"><span class="comment"># 最后返回有宝石和没宝石两种情况的最大值</span></span><br><span class="line"><span class="keyword">return</span> max(d[<span class="number">0</span>][n], d[<span class="number">1</span>][n])</span><br></pre></td></tr></table></figure>
<p>待续…</p>
<h3 id="贪心策略"><a href="#贪心策略" class="headerlink" title="贪心策略"></a>贪心策略</h3><p>待续…</p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>牛客-公平划分</title>
    <url>/20200712_%E5%85%AC%E5%B9%B3%E5%88%92%E5%88%86/index.html</url>
    <content><![CDATA[<p>牛客网系统上的测试用例经常有问题，比如这一题，输入数据有问题。一般情况下，代码完全正确只能通过80%的case。必须对有问题的输入针对性处理才能通过。</p>
<a id="more"></a>

<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>小爱和小溪有N个数字，他们两个想公平的分配这些数字。小爱拿的数字集合为$I = [i_1, i_2, i_K]$，小溪获得剩下的$J=[j_1, j_2, j_{N-K} ]$。但是他们衡量分配公平与否的原则与众不同:<br>$$<br>f(I) = \sum_{i\in I}\sum_{j\in J}|a_i-a_j|<br>$$<br>在小爱拿到其中的K个数字的前提下，计算出他们分配偏差$f(I)$的最小值。</p>
<h2 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述"></a>输入描述</h2><blockquote>
<p>输入第一行两个数字，分别表示总的数字量N和小爱拿的数字量K。第二行有N个数字，表示每个数字的值。</p>
</blockquote>
<h2 id="输出描述"><a href="#输出描述" class="headerlink" title="输出描述"></a>输出描述</h2><blockquote>
<p>输出一个数字，表示分配偏差f(I)的最小值。</p>
</blockquote>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>输入</p>
<blockquote>
<p>4 1</p>
<p>3 3 3 1</p>
</blockquote>
<p>输出</p>
<blockquote>
<p>2</p>
</blockquote>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>牛客官网给的提示是 <strong>动态规划</strong>和<strong>穷举</strong>，我没有想出动态规划的方案，这里用<strong>穷举</strong>的方式可以通过。</p>
<p>对于给定的N个数，小爱同学拿了其中的K个数，那么也就是从N个数中取出K个数，共有$\tbinom{N}{K}$种情况。遍历这$\tbinom{N}{K}$种情况，得到最小的$f(I)$即为最后的答案。</p>
<p>那么问题就转化成了求N个数中取K个数的所有组合情况，本题用combination(data, r)函数，data为list类型数据，r表示从data中取出r个数。combination函数能够返回所有的r个数的组合。关于组合函数combination(data, r)的详解可以看文章<strong>列表排列组合</strong>（撰写中…）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combination</span><span class="params">(data, r)</span>:</span></span><br><span class="line">    n = len(data)</span><br><span class="line">    index = list(range(r))</span><br><span class="line">    <span class="keyword">yield</span> set(index)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(r)):</span><br><span class="line">            <span class="keyword">if</span> index[i] &lt; n - r + i:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        index[i] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, r):</span><br><span class="line">            index[j] = index[j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> set(index)</span><br></pre></td></tr></table></figure>

<p>首先，获得小爱同学的K个数的所有情况sets1 = combination(data, K)，这里combination返回的是python中的生成器，需要通过如下形式迭代形式访问每一种情况：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> set1 <span class="keyword">in</span> sets1:</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<p>这里每一个set1就表示小爱同学的一种取K个数的情况，那么用所有的N个数剔除set1中的数，剩余的数构成set2就是小溪同学的N-K个数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set2 = set(range(N)) - set1</span><br></pre></td></tr></table></figure>

<p>然后求分配偏差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bias = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> set1:</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> set2:</span><br><span class="line">		bias += abs(data[i] - data[j])</span><br></pre></td></tr></table></figure>

<p>判断是否比当前的最小偏差小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min_bias = min(min_bias, bias)</span><br></pre></td></tr></table></figure>

<p>最后输出min_bias。</p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combination</span><span class="params">(data, r)</span>:</span></span><br><span class="line">    n = len(data)</span><br><span class="line">    index = list(range(r))</span><br><span class="line">    <span class="keyword">yield</span> set(index)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(r)):</span><br><span class="line">            <span class="keyword">if</span> index[i] &lt; n - r + i:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        index[i] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, r):</span><br><span class="line">            index[j] = index[j<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">        <span class="keyword">yield</span> set(index)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">########## N, K 输入 ############</span></span><br><span class="line">    line1 = input().strip(<span class="string">''</span>).split(<span class="string">' '</span>)</span><br><span class="line">    N, K = int(line1[<span class="number">0</span>]), int(line1[<span class="number">1</span>])</span><br><span class="line">    <span class="comment">################################</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> N &lt;= <span class="number">0</span> <span class="keyword">or</span> K &lt;= <span class="number">0</span>:</span><br><span class="line">        print(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    line2 = input().strip(<span class="string">''</span>).split(<span class="string">' '</span>)</span><br><span class="line">    data = [int(e) <span class="keyword">for</span> e <span class="keyword">in</span> line2]</span><br><span class="line">    </span><br><span class="line">    min_bias = sys.maxsize</span><br><span class="line">    </span><br><span class="line">    sets1 = combination(data, K)</span><br><span class="line">    <span class="keyword">for</span> set1 <span class="keyword">in</span> sets1:</span><br><span class="line">        set2 = set(range(N)) - set1</span><br><span class="line">        bias = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> set1:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> set2:</span><br><span class="line">                bias += abs(data[i] - data[j])</span><br><span class="line">        min_bias = min(min_bias, bias)</span><br><span class="line">    print(min_bias)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>正常情况下，上述代码是完全正确的。可是有的测试用例有问题，N和K应改是同一行输入的，有的测试用例N和K分两行输入。</p>
<p><strong>正常测试用例：</strong></p>
<blockquote>
<p>N, K</p>
<p>d1 d2 d3 … dN</p>
</blockquote>
<p><strong>异常测试用例：</strong></p>
<blockquote>
<p>N</p>
<p>K</p>
<p>d1 d2 d3 … dN</p>
</blockquote>
<p>因此，如果要提交通过的话，需要将上面N，K输入部分，替换成面的代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dim = list(map(int, input().split()))</span><br><span class="line"><span class="keyword">if</span> len(dim) == <span class="number">2</span>:</span><br><span class="line">	N = dim[<span class="number">0</span>]</span><br><span class="line">	K = dim[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	N = dim[<span class="number">0</span>]</span><br><span class="line">	K = int(input())</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>牛客网</tag>
      </tags>
  </entry>
  <entry>
    <title>智能化教育与隐私保护</title>
    <url>/20201123_%E6%99%BA%E8%83%BD%E5%8C%96%E6%95%99%E8%82%B2%E4%B8%8E%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/index.html</url>
    <content><![CDATA[<p><img src="https://imgs.ebrun.com/resources/2018_01/2018_01_09/2018010914915154810164470.jpg" alt=""></p>
<p>人工智能不仅是一场技术革命和产业革命，更是一场数据革命。为了不断提升智能化水平，必定有越来越多的数据被收集。<a id="more"></a>在智能教育领域，学习者等相关的主体都已被卷入这场声势浩大的数据革命之中。如何在发展智能教育的同时，实现对学习者数据在存储、共享、使用过程中隐私的保护，必定会成为智能教育时代的主题。目前，智能教育和隐私保护之间的平衡性仍处于基础设想阶段。本人及课题组基于教育领域的数据特征，结合隐私保护相关法律法规、伦理道德，界定不同的隐私及隐私程度；从智能教育实施的整个流程所涉及的数据采集、存储与传输、共享与使用等环节来分析隐私泄露的可能性，同时研究具体的实现隐私保护的相关技术。研究结果对于合理发挥教育数据的价值进而提升智能教育水平具有重要意义；为教育的公平性、相关立法、教育领域大数据防火墙的构筑等提供了新的指导。</p>
]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Knowledge Tracing [Pytorch]</title>
    <url>/20201124_DKT-Pytorch/index.html</url>
    <content><![CDATA[<p><img src="http://img.chsong.live/Blogs/DKT-pytorch/1.png-o" alt="DKT"></p>
<p>知识追踪（Knowledge Tracing）是根据学生过去的答题情况对学生的知识掌握情况进行建模，从而得到学生当前知识状态表示的一种技术。将深度学习的方法引入知识追踪最早出现于发表在NeurIPS 2015上的一篇论文《Deep Knowledge Tracing》，作者来自斯坦福大学。在这篇论文中，作者提出了使用深度知识追踪（Deep Knowledge Tracing, DKT）的概念，利用RNN对学生的学习情况进行建模，之后引出了一系列工作，2019年已经有使用Transformer代替RNN和LSTM并且达到了SOTA的论文。DKT作为知识追踪模型深度化的开山之作，在几乎所有的深度知识追踪模型中都作为baseline，而DKT作者给出的模型实现是基于lua语言的，为了能够让更多的研究人员更方便的使用，这里给出一种python的实现，采用的是pytorch框架。</p>
<a id="more"></a>

<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>模型代码已经发布在github上，可点击<a href="https://github.com/chsong513/DeepKnowledgeTracing-DKT-Pytorch" target="_blank" rel="noopener">这里</a>查看和下载具体代码。</p>
<p>或者可以直接通过如下命令直接下载到本地：</p>
<blockquote>
<p>git clone <a href="https://github.com/chsong513/DeepKnowledgeTracing-DKT-Pytorch.git" target="_blank" rel="noopener">https://github.com/chsong513/DeepKnowledgeTracing-DKT-Pytorch.git</a></p>
</blockquote>
<p>具体运行和使用方法参考GitHub项目上ReadMe。</p>
<h2 id="项目结构-DKT"><a href="#项目结构-DKT" class="headerlink" title="项目结构-DKT"></a>项目结构-DKT</h2><p>在DKT文件夹下包括两个文件夹：KTDataset和KnowledgeTracing。</p>
<p><img src="http://img.chsong.live/Blogs/DKT-pytorch/2.png-o" alt=""></p>
<h3 id="数据集-KTDataset"><a href="#数据集-KTDataset" class="headerlink" title="数据集-KTDataset"></a>数据集-KTDataset</h3><p><img src="http://img.chsong.live/Blogs/DKT-pytorch/4.png-o" alt=""></p>
<p>KTDataset文件夹下有6个常用的知识追踪数据集，数据都已经处理成三行格式：</p>
<blockquote>
<p>第一行：答题数<br>第二行：题目编号<br>第三行：答题结果，0表示错，1表示对</p>
</blockquote>
<p>举例：<br><img src="http://img.chsong.live/Blogs/DKT-pytorch/3.png-o" alt=""></p>
<p>Note：可根据需要，按照数据格式自行添加新的数据集。</p>
<h3 id="模型结构-KnowledgeTracing"><a href="#模型结构-KnowledgeTracing" class="headerlink" title="模型结构-KnowledgeTracing"></a>模型结构-KnowledgeTracing</h3><p><img src="http://img.chsong.live/Blogs/DKT-pytorch/5.png-o" alt=""></p>
<p>模型的整个流程都在KnowledgeTracing目录下，包括模型、参数设置、数据处理、模型训练和评估，分别在四个子目录下：model， Constant，data，evaluation。</p>
<h4 id="参数设置-Constant"><a href="#参数设置-Constant" class="headerlink" title="参数设置-Constant"></a>参数设置-Constant</h4><p>Constant下主要设置一些参数和超参数，超参数也分为四大块：数据集存储路径、数据集、题目数、模型超参数。</p>
<p>数据集存储路径</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Dpath = <span class="string">'../../KTDataset'</span></span><br></pre></td></tr></table></figure>
<p>数据集：一共包括6个数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datasets = &#123;</span><br><span class="line">    <span class="string">'assist2009'</span> : <span class="string">'assist2009'</span>,</span><br><span class="line">    <span class="string">'assist2015'</span> : <span class="string">'assist2015'</span>,</span><br><span class="line">    <span class="string">'assist2017'</span> : <span class="string">'assist2017'</span>,</span><br><span class="line">    <span class="string">'static2011'</span> : <span class="string">'static2011'</span>,</span><br><span class="line">    <span class="string">'kddcup2010'</span> : <span class="string">'kddcup2010'</span>,</span><br><span class="line">    <span class="string">'synthetic'</span> : <span class="string">'synthetic'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>题目数：表示每个数据集里面题目的数量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numbers = &#123;</span><br><span class="line">    <span class="string">'assist2009'</span> : <span class="number">124</span>,  </span><br><span class="line">    <span class="string">'assist2015'</span> : <span class="number">100</span>,</span><br><span class="line">    <span class="string">'assist2017'</span> : <span class="number">102</span>,</span><br><span class="line">    <span class="string">'static2011'</span> : <span class="number">1224</span>, </span><br><span class="line">    <span class="string">'kddcup2010'</span> : <span class="number">661</span>,  </span><br><span class="line">    <span class="string">'synthetic'</span> : <span class="number">50</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>模型超参数：主要包括所用数据集、输入输出维度、学习率、最大步长、学习周期等。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DATASET = datasets[<span class="string">'static2011'</span>]</span><br><span class="line">NUM_OF_QUESTIONS = numbers[<span class="string">'static2011'</span>]</span><br><span class="line"><span class="comment"># the max step of RNN model</span></span><br><span class="line">MAX_STEP = <span class="number">50</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR = <span class="number">0.002</span></span><br><span class="line">EPOCH = <span class="number">1000</span></span><br><span class="line"><span class="comment">#input dimension</span></span><br><span class="line">INPUT = NUM_OF_QUESTIONS * <span class="number">2</span></span><br><span class="line"><span class="comment"># embedding dimension</span></span><br><span class="line">EMBED = NUM_OF_QUESTIONS</span><br><span class="line"><span class="comment"># hidden layer dimension</span></span><br><span class="line">HIDDEN = <span class="number">200</span></span><br><span class="line"><span class="comment"># nums of hidden layers</span></span><br><span class="line">LAYERS = <span class="number">1</span></span><br><span class="line"><span class="comment"># output dimension</span></span><br><span class="line">OUTPUT = NUM_OF_QUESTIONS</span><br></pre></td></tr></table></figure>

<h4 id="模型实现-model"><a href="#模型实现-model" class="headerlink" title="模型实现-model"></a>模型实现-model</h4><p>模型在model目录下的RNNModel.py文件中实现，模型实际上就是一个简单的LSTM网络，其结构跟DKT原文中所讲述的结构一致，在LSTM模型最后添加了一个线性层和一个sigmoid激活函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DKT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, layer_dim, output_dim)</span>:</span></span><br><span class="line">        super(DKT, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.layer_dim = layer_dim</span><br><span class="line">        self.output_dim = output_dim</span><br><span class="line">        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=<span class="literal">True</span>,nonlinearity=<span class="string">'tanh'</span>)</span><br><span class="line">        self.fc = nn.Linear(self.hidden_dim, self.output_dim)</span><br><span class="line">        self.sig = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h0 = Variable(torch.zeros(self.layer_dim, x.size(<span class="number">0</span>), self.hidden_dim))</span><br><span class="line">        out,hn = self.rnn(x, h0)</span><br><span class="line">        res = self.sig(self.fc(out))</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<h4 id="数据处理-data"><a href="#数据处理-data" class="headerlink" title="数据处理-data"></a>数据处理-data</h4><p>在data目录下包括三个文件：readdata.py、DKTDataSet.py、dataloader.py。它们的作用分别是定义数据的读取、pytorch框架下的数据集定义、以及pytorch框架下的dataloader的构造。</p>
<p><img src="http://img.chsong.live/Blogs/DKT-pytorch/6.png-o" alt=""></p>
<p><strong>readata</strong>: 在readata.py文件中，定义了一个类：DataReader，从名字可以看出这是一个用来读取数据的类。其中包含两个函数getTrainData()和getTestData()，分别是用来读取训练数据和测试数据。两个函数的定义其实一模一样，只是名字不一样用来区分训练和测试数据，这样的写法有些冗余，后面会再做一些优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataReader</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, path, maxstep, numofques)</span>:</span></span><br><span class="line">        self.path = path</span><br><span class="line">        self.maxstep = maxstep</span><br><span class="line">        self.numofques = numofques</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getTrainData</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getTestData</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>DataReader类有三个参数：</p>
<blockquote>
<p>path: 数据文件存储路径<br>maxstep: 最大序列长度<br>numofques: 此数据集中所有题目的总个数（去重后）</p>
</blockquote>
<p>获取与处理数据部分，以getTrainData()函数为例，getTestData()与其一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTrainData</span><span class="params">(self)</span>:</span></span><br><span class="line">    trainqus = np.array([])</span><br><span class="line">    trainans = np.array([])</span><br><span class="line">    <span class="keyword">with</span> open(self.path, <span class="string">'r'</span>) <span class="keyword">as</span> train:</span><br><span class="line">        <span class="keyword">for</span> len, ques, ans <span class="keyword">in</span> tqdm.tqdm(itertools.zip_longest(*[train] * <span class="number">3</span>), desc=<span class="string">'loading train data:    '</span>, mininterval=<span class="number">2</span>):</span><br><span class="line">            len = int(len.strip().strip(<span class="string">','</span>))</span><br><span class="line">            ques = np.array(ques.strip().strip(<span class="string">','</span>).split(<span class="string">','</span>)).astype(np.int)</span><br><span class="line">            ans = np.array(ans.strip().strip(<span class="string">','</span>).split(<span class="string">','</span>)).astype(np.int)</span><br><span class="line">            mod = <span class="number">0</span> <span class="keyword">if</span> len%self.maxstep == <span class="number">0</span> <span class="keyword">else</span> (self.maxstep - len%self.maxstep)</span><br><span class="line">            zero = np.zeros(mod) - <span class="number">1</span></span><br><span class="line">            ques = np.append(ques, zero)</span><br><span class="line">            ans = np.append(ans, zero)</span><br><span class="line">            trainqus = np.append(trainqus, ques).astype(np.int)</span><br><span class="line">            trainans = np.append(trainans, ans).astype(np.int)</span><br><span class="line">    <span class="keyword">return</span> trainqus.reshape([<span class="number">-1</span>, self.maxstep]), trainans.reshape([<span class="number">-1</span>, self.maxstep])</span><br></pre></td></tr></table></figure>
<p>在getTrainData()中，首先定义两个numpy数组trainqus和trainans，前者存储题目编号，后者存储对应的答题结果。然后打开文件开始读取数据。</p>
<p>因为数据是三行格式的，所以每一次读取三行，每次读取三行的实现方式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> len, ques, ans <span class="keyword">in</span> tqdm.tqdm(itertools.zip_longest(*[train] * <span class="number">3</span>), desc=<span class="string">'loading train data:    '</span>, mininterval=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>其中tqdm是进度条展示，可忽略，简化来看每次读取三行的方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> len, ques, ans <span class="keyword">in</span> itertools.zip_longest(*[train] * <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>然后是对三行数据进行字符串处理，分别得到题目编号以及对应的答题结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ques = np.array(ques.strip().strip(<span class="string">','</span>).split(<span class="string">','</span>)).astype(np.int)</span><br><span class="line">ans = np.array(ans.strip().strip(<span class="string">','</span>).split(<span class="string">','</span>)).astype(np.int)</span><br></pre></td></tr></table></figure>
<p>然后是处理长度不一致的问题，将所有答题序列的长度都处理成maxstep的整数倍，长度不够的补0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mod = <span class="number">0</span> <span class="keyword">if</span> len%self.maxstep == <span class="number">0</span> <span class="keyword">else</span> (self.maxstep - len%self.maxstep)</span><br><span class="line">zero = np.zeros(mod) - <span class="number">1</span></span><br><span class="line">ques = np.append(ques, zero)</span><br><span class="line">ans = np.append(ans, zero)</span><br></pre></td></tr></table></figure>
<p>举例：ques长度为18，设置maxstep为5，那么ques补充成maxstep的整数倍应该是4倍为20，所以ques应该补充两个0变成长度为20的序列；如果ques长度为11，那么补充4个0，长度变成15；ques长度为10，则不补充。</p>
<p>每一个ques的长度处理成maxstep的整数倍之后，添加到trainques数组中去，这样每一次添加都保证了trainques的长度为maxstep的整数倍。ans以及trainans的处理过程一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainqus = np.append(trainqus, ques).astype(np.int)</span><br><span class="line">trainans = np.append(trainans, ans).astype(np.int)</span><br></pre></td></tr></table></figure>
<p>最后对trainques和trainans进行reshape，处理成N*maxstep的矩阵形式，N即可看做学生个数。maxstep即为答题个数。</p>
<p>举例，数据形式的变化过程，比如设置maxstep为3，总题目数为5，现在有如下三个学生的原始答题记录：<br>学生1：<br>2<br>1 2<br>1 0<br>学生2：<br>4<br>2 4 1 3<br>0 1 1 0<br>学生3：<br>7<br>5 3 1 4 5 4 2<br>0 0 1 1 0 1 0</p>
<p>ques通过readata读取并处理之后会变成：<br>1 2 0<br>2 4 1<br>3 0 0<br>5 3 1<br>4 5 4<br>2 0 0</p>
<p><strong>DKTDataSet</strong>：要定义pytorch框架下的数据集，需要继承torch的Dataset类，覆写__init__、__len__以及__getitem__三个函数。还可以根据需要自己添加数据处理的函数，在DKTDataSet中添加的one-hot处理函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DKTDataSet</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ques, ans)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">onehot</span><span class="params">(self, questions, answers)</span>:</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在readdata处理好数据之后，我们在DKTDataSet中对其进行封装处理，直接返回题目的one-hot形式而不再是题目编号。</p>
<p>在__init__中做一些初始化操作，比入读进数据ques和ans，前者是题目编号，后者是答题结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ques, ans)</span>:</span></span><br><span class="line">    self.ques = ques</span><br><span class="line">    self.ans = ans</span><br></pre></td></tr></table></figure>

<p>__len__返回数据集的长度（大小），这里直接返回ques或者ans的行数，也就是学生数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.ques)</span><br></pre></td></tr></table></figure>

<p>__getitem__返回需要获取的某条数据，这里根据index参数直接返回对应的数据即可，这里我们返回前将数据通过自定义的onehot函数处理成one-hot的形式，并且将数据类型转换为FloatTensor。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    questions = self.ques[index]</span><br><span class="line">    answers = self.ans[index]</span><br><span class="line">    onehot = self.onehot(questions, answers)</span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(onehot.tolist())</span><br></pre></td></tr></table></figure>

<p>__onehot__是自定义的将题目编号转变成one-hot形式的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onehot</span><span class="params">(self, questions, answers)</span>:</span></span><br><span class="line">    result = np.zeros(shape=[C.MAX_STEP, <span class="number">2</span> * C.NUM_OF_QUESTIONS])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(C.MAX_STEP):</span><br><span class="line">        <span class="keyword">if</span> answers[i] &gt; <span class="number">0</span>:</span><br><span class="line">            result[i][questions[i]] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> answers[i] == <span class="number">0</span>:</span><br><span class="line">            result[i][questions[i] + C.NUM_OF_QUESTIONS] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>与原文保持一致，one-hot的维度为两倍的总题目数，所以对于readata中处理好的每一条记录ques，将变成[C.MAX_STEP, 2 * C.NUM_OF_QUESTIONS]大小的矩阵，因为每条记录ques中包含C.MAX_STEP个题目，每个题目的onehot维度为2 * C.NUM_OF_QUESTIONS。</p>
<p>接着readata中的例子，ques在DKTDataSet中转变成onehot形式之后，数据的形式变成：<br>[[1 0 0 0 0 0 0 0 0 0] -&gt; 1<br>&nbsp;[0 0 0 0 0 0 1 0 0 0] -&gt; 2<br>&nbsp;[0 0 0 0 0 0 0 0 0 0] -&gt; 0<br>&nbsp;[0 0 0 0 0 0 1 0 0 0] -&gt; 2<br>&nbsp;[0 0 0 1 0 0 0 0 0 0] -&gt; 4<br>&nbsp;[1 0 0 0 0 0 0 0 0 0] -&gt; 1<br>&nbsp;…]</p>
<p><strong>dataloader</strong>：在dataloader.py中，包含一个训练数据的loader和一个测试数据的loader，分别是getTrainLoader和getTestLoader，实际上这两个loader的实现一模一样，只是去了两个不同的名字为了区分训练和测试数据，这样的方式比较冗余，后面的版本会进行优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTrainLoader</span><span class="params">(train_data_path)</span>:</span></span><br><span class="line">    handle = DataReader(train_data_path ,C.MAX_STEP, C.NUM_OF_QUESTIONS)</span><br><span class="line">    trainques, trainans = handle.getTrainData()</span><br><span class="line">    dtrain = DKTDataSet(trainques, trainans)</span><br><span class="line">    trainLoader = Data.DataLoader(dtrain, batch_size=C.BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> trainLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTestLoader</span><span class="params">(test_data_path)</span>:</span></span><br><span class="line">    handle = DataReader(test_data_path, C.MAX_STEP, C.NUM_OF_QUESTIONS)</span><br><span class="line">    testques, testans = handle.getTestData()</span><br><span class="line">    dtest = DKTDataSet(testques, testans)</span><br><span class="line">    testLoader = Data.DataLoader(dtest, batch_size=C.BATCH_SIZE, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> testLoader</span><br></pre></td></tr></table></figure>
<p>关于如何定义loader就不做过多介绍，关于pytorch的dataloader的相关文章有很多。</p>
<p>在dataloader.py中还有一个函数：getLoader，这个函数封装了getTrainLoader和getTestLoader，通过调用此函数直接获取训练和测试的loader。并且函数的参数是数据集的名称，根据数据集名称分别为不同的数据集构造loader。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLoader</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    trainLoaders = []</span><br><span class="line">    testLoaders = []</span><br><span class="line">    <span class="keyword">if</span> dataset == <span class="string">'assist2009'</span>:</span><br><span class="line">        trainLoader = getTrainLoader(C.Dpath + <span class="string">'/assist2009/builder_train.csv'</span>)</span><br><span class="line">        trainLoaders.append(trainLoader)</span><br><span class="line">        testLoader = getTestLoader(C.Dpath + <span class="string">'/assist2009/builder_test.csv'</span>)</span><br><span class="line">        testLoaders.append(testLoader)</span><br><span class="line">    <span class="keyword">elif</span> dataset == <span class="string">'assist2015'</span>:</span><br><span class="line">        trainLoader = getTrainLoader(C.Dpath + <span class="string">'/assist2015/assist2015_train.txt'</span>)</span><br><span class="line">        trainLoaders.append(trainLoader)</span><br><span class="line">        testLoader = getTestLoader(C.Dpath + <span class="string">'/assist2015/assist2015_test.txt'</span>)</span><br><span class="line">        testLoaders.append(testLoader)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<h4 id="模型训练与测试-evaluation"><a href="#模型训练与测试-evaluation" class="headerlink" title="模型训练与测试-evaluation"></a>模型训练与测试-evaluation</h4><p>在evaluation目录下，有两个文件，一个是eval.py文件，主要实现模型的训练和测试以及品谷的过程；另一个是run.py文件，是主程序入口。</p>
<p><img src="http://img.chsong.live/Blogs/DKT-pytorch/7.png-o" alt=""></p>
<p><strong>eval</strong>：在eval.py文件中，定义了两个函数train和test分别实现模型的训练和测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(trainLoaders, model, optimizer, lossFunc)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(trainLoaders)):</span><br><span class="line">        model, optimizer = train_epoch(model, trainLoaders[i], optimizer, lossFunc)</span><br><span class="line">    <span class="keyword">return</span> model, optimizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(testLoaders, model)</span>:</span></span><br><span class="line">    ground_truth = torch.Tensor([])</span><br><span class="line">    prediction = torch.Tensor([])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testLoaders)):</span><br><span class="line">        pred_epoch, gold_epoch = test_epoch(model, testLoaders[i])</span><br><span class="line">        prediction = torch.cat([prediction, pred_epoch])</span><br><span class="line">        ground_truth = torch.cat([ground_truth, gold_epoch])</span><br><span class="line">    performance(ground_truth, prediction)</span><br></pre></td></tr></table></figure>
<p>而训练过程有分为很多epoch，每一个epoch的过程在train_epoch中实现。而对于测试过程，由于某些测试集可能会很大，导致内存一次存不下，所以将测试集分成多个loader，然后对于每一个loader都调用一次test_epoch，然后把所有的loader的结果合并起来。最后，所有的结果拼接起来后，通过performance函数计算模型的各个评价指标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">    prediction = torch.cat([prediction, pred_epoch])</span><br><span class="line">    ground_truth = torch.cat([ground_truth, gold_epoch])</span><br><span class="line">performance(ground_truth, prediction)</span><br></pre></td></tr></table></figure>
<p>对于train_epoch，过程跟一般的pytorch模型训练过程一样，读取数据loader、预测、计算损失、反向传播等：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_epoch</span><span class="params">(model, trainLoader, optimizer, loss_func)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm.tqdm(trainLoader, desc=<span class="string">'Training:    '</span>, mininterval=<span class="number">2</span>):</span><br><span class="line">        pred = model(batch)</span><br><span class="line">        loss = loss_func(pred, batch)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">return</span> model, optimizer</span><br></pre></td></tr></table></figure>
<p>对于test_epoch，由于知识追踪任务比较特殊，每一个时刻的输出都是预测下一个时刻答对题目的概率，因此有一些额外的处理。先上代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_epoch</span><span class="params">(model, testLoader)</span>:</span></span><br><span class="line">    gold_epoch = torch.Tensor([])</span><br><span class="line">    pred_epoch = torch.Tensor([])</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm.tqdm(testLoader, desc=<span class="string">'Testing:    '</span>, mininterval=<span class="number">2</span>):</span><br><span class="line">        pred = model(batch)</span><br><span class="line">        <span class="keyword">for</span> student <span class="keyword">in</span> range(pred.shape[<span class="number">0</span>]):</span><br><span class="line">            temp_pred = torch.Tensor([])</span><br><span class="line">            temp_gold = torch.Tensor([])</span><br><span class="line">            delta = batch[student][:,<span class="number">0</span>:C.NUM_OF_QUESTIONS] + batch[student][:,C.NUM_OF_QUESTIONS:]</span><br><span class="line">            temp = pred[student][:C.MAX_STEP - <span class="number">1</span>].mm(delta[<span class="number">1</span>:].t())</span><br><span class="line">            index = torch.LongTensor([[i <span class="keyword">for</span> i <span class="keyword">in</span> range(C.MAX_STEP - <span class="number">1</span>)]])</span><br><span class="line">            p = temp.gather(<span class="number">0</span>, index)[<span class="number">0</span>]</span><br><span class="line">            a = (((batch[student][:, <span class="number">0</span>:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(<span class="number">1</span>) + <span class="number">1</span>)//<span class="number">2</span>)[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)):</span><br><span class="line">                <span class="keyword">if</span> p[i] &gt; <span class="number">0</span>:</span><br><span class="line">                    temp_pred = torch.cat([temp_pred,p[i:i+<span class="number">1</span>]])</span><br><span class="line">                    temp_gold = torch.cat([temp_gold, a[i:i+<span class="number">1</span>]])</span><br><span class="line">            pred_epoch = torch.cat([pred_epoch, temp_pred])</span><br><span class="line">            gold_epoch = torch.cat([gold_epoch, temp_gold])</span><br><span class="line">    <span class="keyword">return</span> pred_epoch, gold_epoch</span><br></pre></td></tr></table></figure>
<p>在test_epoch函数中，先定义两个列表，分别用来存储真实结果ground truth 和预测的结果pred：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gold_epoch = torch.Tensor([])</span><br><span class="line">pred_epoch = torch.Tensor([])</span><br></pre></td></tr></table></figure>
<p>然后读取数据，分多个batch进行预测，因为一次预测可能数据量过大导致内存溢出而出错。Note：每一个batch中包含多个学生，每个学生有maxstep个题目，每个题目表示成了2*num_of_ques维的onehot向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> tqdm.tqdm(testLoader, desc=<span class="string">'Testing:    '</span>, mininterval=<span class="number">2</span>):</span><br><span class="line">    pred = model(batch)</span><br></pre></td></tr></table></figure>
<p>预测完之后，整理数据，把学生所有的题目的预测结果存储起来，方便后面的评估。对于每一个学生，先创建两个列表，分别存储真是答题结果ground truth和预测结果pred。然后再将每个学生的结果添加进开始定义的两个总结果列表gold_epoch和pred_epoch中去。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> student <span class="keyword">in</span> range(pred.shape[<span class="number">0</span>]):</span><br><span class="line">    temp_pred = torch.Tensor([])</span><br><span class="line">    temp_gold = torch.Tensor([])</span><br></pre></td></tr></table></figure>
<p>然后是获取预测结果，这里先将2*num_of_ques维的题目onehot向量分成前后两个部分，每部分分别是num_of_ques维，然后相加，乘以预测结果，即可得到对应的题目的预测结果，这里的计算过程可自行推敲，等有机会再给出可视化的计算过程。因为每一个时刻都是预测的下一个时刻的结果，所以题目编号需要向后移一个，体现在delta[1:]这里：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">delta = batch[student][:,<span class="number">0</span>:C.NUM_OF_QUESTIONS] + batch[student][:,C.NUM_OF_QUESTIONS:]</span><br><span class="line">temp = pred[student][:C.MAX_STEP - <span class="number">1</span>].mm(delta[<span class="number">1</span>:].t())</span><br><span class="line">index = torch.LongTensor([[i <span class="keyword">for</span> i <span class="keyword">in</span> range(C.MAX_STEP - <span class="number">1</span>)]])</span><br><span class="line">p = temp.gather(<span class="number">0</span>, index)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>对于答题的真实结果，其实在onehot的向量中就已经体现了，答对则向量前半部分对应的位置为1，答错则向量后半部分对应的位置为1。根据这个特点，按照下面的方式就可以直接通过onehot向量推出真实答题结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = (((batch[student][:, <span class="number">0</span>:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(<span class="number">1</span>) + <span class="number">1</span>)//<span class="number">2</span>)[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<p>到此处为止，预测结果和真实结果就已经都得到了。但是，这里还要在做一个筛选，别忘了我们之前在数据长度不够的时候是补0了的，这里需要把补0的结果全部都过滤掉。由于补零的题目的onehot向量为全零向量，那么全零向量经过神经网络之后预测结果肯定为0。而正常题目不是非零的，那么预测结果为0的可能性极小，因为神经网络参数为0的可能性极小。所以我们根据预测结果是否为0，直接把为0的全部去除掉（我们这里的处理方法似乎不是很合理，因为正常题目也是有可能出现预测结果为0的情况，但是这种可能性极小，对模型整体而言几乎没什么影响，所以这么做也是合理的，并且十分方便）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> p[i] &gt; <span class="number">0</span>:</span><br><span class="line">    temp_pred = torch.cat([temp_pred,p[i:i+<span class="number">1</span>]])</span><br><span class="line">    temp_gold = torch.cat([temp_gold, a[i:i+<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<p>在每次处理完一个学生的数据之后，将其添加到总结果列表中去：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_epoch = torch.cat([pred_epoch, temp_pred])</span><br><span class="line">gold_epoch = torch.cat([gold_epoch, temp_gold])</span><br></pre></td></tr></table></figure>
<p>最后返回结果即可。</p>
<p>在eval.py文件中还定义了一个损失函数类lossFunc，基于pytorch框架的自定义的损失函数。其实这个损失函数就是分类问题中常用的交叉熵函数，只是知识追踪问题的数据是序列化的，所以这里不太方便直接调用pytorch框架中已有的交叉熵函数，自己按需实现了一下，里面涉及的一些过程和test_epoch中的部分过程类似：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lossFunc</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(lossFunc, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, batch)</span>:</span></span><br><span class="line">        loss = torch.Tensor([<span class="number">0.0</span>])</span><br><span class="line">        <span class="keyword">for</span> student <span class="keyword">in</span> range(pred.shape[<span class="number">0</span>]):</span><br><span class="line">            delta = batch[student][:,<span class="number">0</span>:C.NUM_OF_QUESTIONS] + batch[student][:,C.NUM_OF_QUESTIONS:]</span><br><span class="line">            temp = pred[student][:C.MAX_STEP - <span class="number">1</span>].mm(delta[<span class="number">1</span>:].t())</span><br><span class="line">            index = torch.LongTensor([[i <span class="keyword">for</span> i <span class="keyword">in</span> range(C.MAX_STEP - <span class="number">1</span>)]])</span><br><span class="line">            p = temp.gather(<span class="number">0</span>, index)[<span class="number">0</span>]</span><br><span class="line">            a = (((batch[student][:, <span class="number">0</span>:C.NUM_OF_QUESTIONS] - batch[student][:, C.NUM_OF_QUESTIONS:]).sum(<span class="number">1</span>) + <span class="number">1</span>)//<span class="number">2</span>)[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)):</span><br><span class="line">                <span class="keyword">if</span> p[i] &gt; <span class="number">0</span>:</span><br><span class="line">                    loss = loss - (a[i]*torch.log(p[i]) + (<span class="number">1</span>-a[i])*torch.log(<span class="number">1</span>-p[i]))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>最后，eval.py文件中包含一个performance函数，从名字就可以看出这个函数用来评价模型的表现，也就是计算预测结果的各个指标，包括AUC、F1、Recall、Precision，可以根据需要自行添加，计算方式可自定义或者直接掉包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">performance</span><span class="params">(ground_truth, prediction)</span>:</span></span><br><span class="line">    fpr, tpr, thresholds = metrics.roc_curve(ground_truth.detach().numpy(), prediction.detach().numpy())</span><br><span class="line">    auc = metrics.auc(fpr, tpr)</span><br><span class="line">    f1 = metrics.f1_score(ground_truth.detach().numpy(), torch.round(prediction).detach().numpy())</span><br><span class="line">    recall = metrics.recall_score(ground_truth.detach().numpy(), torch.round(prediction).detach().numpy())</span><br><span class="line">    precision = metrics.precision_score(ground_truth.detach().numpy(), torch.round(prediction).detach().numpy())</span><br><span class="line">    print(<span class="string">'auc:'</span> + str(auc) + <span class="string">' f1: '</span> + str(f1) + <span class="string">' recall: '</span> + str(recall) + <span class="string">' precision: '</span> + str(precision) + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>到此处为止，DKT项目的所有部分都已介绍完毕。由于时间仓促，并没有把所有细节都介绍很清楚，但对于学习和理解DKT来说已经足够了。后续有时间会根据需要补充一些更细节的介绍，如果有什么问题或建议可直接评论留言，我会及时回复，或者通过主页的邮箱联系。</p>

<font color=red>这里有一个知识追踪交流群，欢迎大家加群交流学习。二维码如过期，请直接在评论区提醒，会及时更新。

未及时更新，也可以直接扫描主页二维码加我，拉你进群。
</font>

<p><img src="https://img.chsong.live/Blogs/DKT-pytorch/DKT%E4%BA%A4%E6%B5%81%E7%BE%A4.png-m" alt=""></p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA与SVD之间的关系</title>
    <url>/20201205_PCA%E4%B8%8ESVD%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/index.html</url>
    <content><![CDATA[<p><img src="http://img.chsong.live/Blogs/PCA%E4%B8%8ESVD%E7%9A%84%E5%85%B3%E7%B3%BB/1.png-s" alt="PCA"></p>
<p>在用数据对模型进行训练时，通常会遇到维度过高，也就是数据的特征太多的问题，有时特征之间还存在一定的相关性，这时如果还使用原数据训练模型，模型的精度会大大下降，因此要降低数据的维度，同时新数据的特征之间还要保持线性无关。有一种方法称为主成分分析（Principal component analysis，PCA），新数据的特征称为主成分，得到主成分的方法有两种：直接对协方差矩阵进行特征值分解和对数据矩阵进行奇异值分解（SVD）。</p>
<a id="more"></a>

<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>一种经典的数据降维算法，推导过程有多种形式：最近重构性、最大可区分性等。这里以最大可区分性为例。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>设$X\in R^{n\times d} = {x_1, x_2,\dots, x_n}$为数据集，包含$n$个样本，每个样本有$d$维特征, 即$x_i\in R^{d}$。</p>
<h3 id="降维可视化"><a href="#降维可视化" class="headerlink" title="降维可视化"></a>降维可视化</h3><p><img src="http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279401413/1576909454842_sVWwhScNGQ.jpg" alt=""></p>
<p>降维的目的，是为了找到一个映射矩阵$W\in R^{k\times d}$,将原始数据都映射到d维空间中。</p>
<p>如图所示，以2维空间到1维空间降维为例。原数据在直线上的映射点的位置之与直线的斜率（或者说是方向向量）有关，且直线是可以平移的。因此我们假设目标直线是过原点的，即：$Wx = 0$。</p>
<h3 id="W求解过程"><a href="#W求解过程" class="headerlink" title="W求解过程"></a>W求解过程</h3><p>每一个数据点$x_i$映射到直线$Wx=0$上之后，映射点为：$Wx_i$</p>
<p>那么数据集$X$映射后的数据为:$XW^T\in R^{n\times k}$</p>
<p>step1: 映射矩阵$W\in R^{k\times d}$将数据集$X$映射到$k$维空间：$XW^{T}$</p>
<p>step2: 映射后，数据的协方差矩阵为：$(XW^{T})^{T}(XW^{T})=WX^{T}XW^{T}$</p>
<p>最大可区分性，就是使得数据在各个维度上的方差最大，也就是数据最分散，越分散区分性越大。因此最大可区分性的优化目标就是，数据集在各位维度上的方差最大化。而各个维度的方差等于协方差矩阵的对角元素之和，也就是矩阵的迹。</p>
<p>step3: 因此优化目标：$min \text{tr}(WX^{T}XW^{T}), s.t. WW^T = I$</p>
<p>step4: 拉格朗日法求解，可得：$X^TXW^T = \lambda W$</p>
<p>$W$的解为矩阵$X^TX$的特征向量。</p>
<h3 id="与SVD的关系"><a href="#与SVD的关系" class="headerlink" title="与SVD的关系"></a>与SVD的关系</h3><p>奇异值分解，对于矩阵X，其奇异值分解为: $X = U\Sigma V^T$</p>
<p>而矩阵$X^TX = (U\Sigma V)^TU\Sigma V^T = V\Sigma U^TU\Sigma V^T = V\Sigma^2 V^T$</p>
<p>因此矩阵$X^TX$的特征值构成的矩阵$V$即为矩阵$X$奇异值分解后的右特征矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris[<span class="string">'data'</span>]</span><br><span class="line">Y = iris[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''normalization mean = 0, std = 1'''</span></span><br><span class="line">X_normed = (X - X.mean(axis=<span class="number">0</span>)) / X.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''SVD of X_normed'''</span></span><br><span class="line">U, Sigma, VT = np.linalg.svd(X_normed)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''take the vector from V'''</span></span><br><span class="line">W = VT.T[:<span class="number">2</span>,:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''conduct PCA 降维'''</span></span><br><span class="line">X_pca = X_normed.dot(W.T)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''visualization'''</span></span><br><span class="line">plt.scatter(X_pca[:,<span class="number">0</span>],X_pca[:,<span class="number">1</span>], c=Y)</span><br></pre></td></tr></table></figure>

<p><img src="http://img.chsong.live/Blogs/PCA%E4%B8%8ESVD%E7%9A%84%E5%85%B3%E7%B3%BB/2.png-s" alt="png"></p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>游戏实时胜率预测</title>
    <url>/20201123_%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/index.html</url>
    <content><![CDATA[<p><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/winprob.png-o" alt=""></p>
<p>在游戏过程中进行实时胜率预测，能够为观战者、解说提供话题引导和需求。实时胜率预测系统目前 在游戏赛事的观战直播领域已经出现一些应用尝试。肯德基KI上校、DOTA PLUS胜率预测面板，已经在LOL和DOTA玩家中取得了良好的反馈。通过战斗的实时胜率预测，不仅可以制造话题为比赛带来乐趣，也可以通过事后回顾，分析对局中的得失，提升竞技实力，对游戏玩法参与感和体验，是一种革新。</p>
<a id="more"></a>

<p>本文是一个关于游戏实时胜率预测ppt的介绍，这个ppt是我在实验室一次大分享上的报告。此ppt主要包括四个部分：背景介绍、相关工作、我们的工作、以及总结。其中我们的工作，是我之前在网易实习时所作的一项研究，是关于<a href="http://n.163.com" target="_blank" rel="noopener">逆水寒</a>游戏实时胜率预测的工作。</p>
<table>
<thead>
<tr>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/1.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/2.png-m" alt=""></th>
</tr>
</thead>
</table>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>首先是介绍了一下游戏背景部分，主要包括当前全球游戏市场的现状、游戏对学术界的影响、游戏在产业界的应用。然后引出实时胜率预测的应用及其重要性，最后总结实时胜率预测及其相关的延伸问题。</p>
<table>
<thead>
<tr>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/3.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/4.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/5.png-m" alt=""></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/6.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/7.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/8.png-m" alt=""></td>
</tr>
</tbody></table>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>然后是相关工作部分，从两个方面来介绍。一方面是简要回顾了一下传统的统计学方法比如BT算法、ELo机制等。另一方面是介绍基于深度学习的方法。挑了三个比较有代表性的研究进行相对详细的介绍，一个是基于英雄联盟游戏视频的高光时刻生成、一个是基于王者荣耀游戏视频的无监督高光片段检测框架、另一个是足球运动员动作价值评估。</p>
<table>
<thead>
<tr>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/9.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/10.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/11.png-m" alt=""></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/12.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/13.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/14.png-m" alt=""></td>
</tr>
<tr>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/15.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/16.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/17.png-m" alt=""></td>
</tr>
</tbody></table>
<h2 id="我们的工作"><a href="#我们的工作" class="headerlink" title="我们的工作"></a>我们的工作</h2><p>我们的工作主要是基于逆水寒游戏数据，研究实时胜率预测模型。所作的工作主要包括三个步骤，首先基于shapely value对数据进行了分析，分析游戏中不同角色的数量对游戏结果的影响；然后采用LR、XGboost等方法进行实时胜率预测实验，效果挺好，但是可解释性不足。后来将数据处理成图片形式，还原游戏对局原貌，采用CNN实现胜率预测，并通过热力图提供强解释性。</p>
<table>
<thead>
<tr>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/18.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/19.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/20.png-m" alt=""></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/21.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/22.png-m" alt=""></td>
<td align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/23.png-m" alt=""></td>
</tr>
</tbody></table>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后总结了一下基于游戏实时胜率预测的可能的研究方向，主要包括：关键点识别、高光时刻检测、实时胜率的可解释性、战报自动生成、战场数据表征等。</p>
<table>
<thead>
<tr>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/24.png-m" alt=""></th>
<th align="center"><img src="https://img.chsong.live/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/25.png-m" alt=""></th>
</tr>
</thead>
</table>
<p>原ppt可以在<a href="http://chengsong-img.oss-cn-hangzhou.aliyuncs.com/Blogs/%E6%B8%B8%E6%88%8F%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B/20201015%E5%A4%A7%E5%88%86%E4%BA%AB%E5%AE%9E%E6%97%B6%E8%83%9C%E7%8E%87%E9%A2%84%E6%B5%8B.pptx" target="_blank" rel="noopener">此处</a>下载。</p>
]]></content>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>再见</title>
    <url>/20201103_%E5%86%8D%E8%A7%81/index.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">您好, 这里需要密码.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="bf66d1a238f8c7953e6e7753a591a33d504129fd40acffdf0bc8eba7b8cff824">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>随笔</tag>
        <tag>动态</tag>
      </tags>
  </entry>
  <entry>
    <title>Transfer Component Analyze (TCA)</title>
    <url>/20201205_TCA/index.html</url>
    <content><![CDATA[<p><img src="http://img.chsong.live/Blogs/TCA/1.png-s" alt="TCA"></p>
<p>TCA属于基于特征的迁移学习方法。那么，它做了一件什么事呢？用通俗的语言来说，跟PCA很像：PCA是一个大矩阵进去，一个小矩阵出来，TCA呢，是两个大矩阵进去，两个小矩阵出来。从学术角度讲，TCA针对domain adaptation问题中，源域和目标域处于不同数据分布时，将两个领域的数据一起映射到一个高维的再生核希尔伯特空间。在此空间中，最小化源和目标的数据距离，同时最大程度地保留它们各自的内部属性。直观地理解就是，在现在这个维度上不好最小化它们的距离，那么我就找个映射，在映射后的空间上让它们最接近，那么我不就可以进行分类了吗。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.mlab <span class="keyword">as</span> mlab</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<p>本文是在阿里天池实验室平台上实现的，数据集是我自己随机生成的模拟数据集。关于数据集的信息可以在<a href="https://tianchi.aliyun.com/notebook-ai/home#datasetLabId=48363&operaType=2" target="_blank" rel="noopener">此处</a>查看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source = pd.read_csv(<span class="string">'datalab/48363/TCA_source.csv'</span>, sep = <span class="string">','</span>).values</span><br><span class="line">target = pd.read_csv(<span class="string">'datalab/48363/TCA_target.csv'</span>, sep = <span class="string">','</span>).values</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(source[<span class="number">0</span>:<span class="number">100</span>,<span class="number">2</span>],source[<span class="number">0</span>:<span class="number">100</span>,<span class="number">3</span>], c=<span class="string">''</span>,marker=<span class="string">'o'</span>, alpha=<span class="number">0.7</span>, edgecolors=<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(source[<span class="number">100</span>:,<span class="number">2</span>],source[<span class="number">100</span>:,<span class="number">3</span>], c=<span class="string">''</span>,marker=<span class="string">'*'</span>, alpha=<span class="number">0.7</span>, edgecolors=<span class="string">'r'</span>)</span><br><span class="line">plt.scatter(target[<span class="number">0</span>:<span class="number">100</span>,<span class="number">2</span>],target[<span class="number">0</span>:<span class="number">100</span>,<span class="number">3</span>], c=<span class="string">''</span>,marker=<span class="string">'^'</span>, alpha=<span class="number">0.7</span>, edgecolors=<span class="string">'b'</span>)</span><br><span class="line">plt.scatter(target[<span class="number">100</span>:,<span class="number">2</span>],target[<span class="number">100</span>:,<span class="number">3</span>], c=<span class="string">''</span>,marker=<span class="string">'s'</span>, alpha=<span class="number">0.7</span>, edgecolors=<span class="string">'b'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">plt.legend((<span class="string">'Pos. Source'</span>, <span class="string">'Neg. Source'</span>, <span class="string">'Pos. Target'</span>, <span class="string">'Neg. Target'</span>))</span><br><span class="line">```  </span><br><span class="line">![png](http://img.chsong.live/Blogs/TCA/<span class="number">2.</span>png-s)</span><br><span class="line">```python</span><br><span class="line">Source_X = source[:, <span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">Source_Y = source[:,<span class="number">1</span>]</span><br><span class="line">Target_X = target[:, <span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">Target_Y = target[:, <span class="number">1</span>]</span><br><span class="line">print(Source_X.shape, Source_Y.shape, Target_X.shape, Target_Y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(200, 2) (200,) (200, 2) (200,)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''PCA'''</span></span><br><span class="line"><span class="comment"># center matrix for cov</span></span><br><span class="line">ns = nt = <span class="number">200</span></span><br><span class="line">n = ns + nt</span><br><span class="line">H = np.eye(n) - (<span class="number">1</span>/n) * np.ones((n,n))</span><br><span class="line">All_X = np.concatenate((Source_X, Target_X), axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># All_X = (All_X - All_X.mean(axis=0)) / All_X.std(axis=0)  #归一化操作，可加可不加</span></span><br><span class="line">XHX = All_X.T.dot(H).dot(All_X)</span><br><span class="line">eig_values, eig_vectors = np.linalg.eig(XHX)</span><br><span class="line">print(eig_vectors)</span><br><span class="line">W = eig_vectors[:,<span class="number">1</span>][:,np.newaxis]</span><br><span class="line">print(W)</span><br></pre></td></tr></table></figure>
<pre><code>[[-0.82451726 -0.56583681]
 [ 0.56583681 -0.82451726]]
[[-0.56583681]
 [-0.82451726]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">All_X_PCA = W.T.dot(All_X.T).squeeze()</span><br><span class="line"><span class="comment">#################两个领域整体数据分布</span></span><br><span class="line">plt.figure()</span><br><span class="line">S_n, S_bins, S_patches = plt.hist(All_X_PCA[<span class="number">0</span>:<span class="number">200</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">T_n, T_bins, T_patches = plt.hist(All_X_PCA[<span class="number">200</span>: ], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.title(<span class="string">'Principle Component Analysis'</span>)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.ylabel(<span class="string">'PDF'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">S_mu, S_sigma = All_X_PCA[<span class="number">0</span>:<span class="number">200</span>].mean(), All_X_PCA[<span class="number">0</span>:<span class="number">200</span>].std()</span><br><span class="line">T_mu, T_sigma = All_X_PCA[<span class="number">200</span>: ].mean(), All_X_PCA[<span class="number">200</span>: ].std()</span><br><span class="line">S_y, T_y = mlab.normpdf(S_bins, S_mu, S_sigma), mlab.normpdf(T_bins, T_mu, T_sigma)</span><br><span class="line"></span><br><span class="line">plt.plot(S_bins, S_y, <span class="string">'r-'</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">plt.plot(T_bins, T_y, <span class="string">'b-'</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##################两个领域，正负类别数据的分布</span></span><br><span class="line">plt.figure()</span><br><span class="line">S_P_n, S_P_bins, S_P_patches = plt.hist(All_X_PCA[<span class="number">0</span>:<span class="number">100</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">S_N_n, S_N_bins, S_N_patches = plt.hist(All_X_PCA[<span class="number">100</span>:<span class="number">200</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">T_P_n, T_P_bins, T_P_patches = plt.hist(All_X_PCA[<span class="number">200</span>:<span class="number">300</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">T_N_n, T_N_bins, T_N_patches = plt.hist(All_X_PCA[<span class="number">300</span>: ], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)</span><br><span class="line">plt.close()</span><br><span class="line"></span><br><span class="line">S_P_mu, S_P_sigma = All_X_PCA[<span class="number">0</span>:<span class="number">100</span>].mean(), All_X_PCA[<span class="number">0</span>:<span class="number">100</span>].std()</span><br><span class="line">S_N_mu, S_N_sigma = All_X_PCA[<span class="number">100</span>:<span class="number">200</span>].mean(), All_X_PCA[<span class="number">100</span>:<span class="number">200</span>].std()</span><br><span class="line">T_P_mu, T_P_sigma = All_X_PCA[<span class="number">200</span>:<span class="number">300</span>].mean(), All_X_PCA[<span class="number">200</span>:<span class="number">300</span>].std()</span><br><span class="line">T_N_mu, T_N_sigma = All_X_PCA[<span class="number">300</span>: ].mean(), All_X_PCA[<span class="number">300</span>: ].std()</span><br><span class="line">S_P_y, T_P_y = mlab.normpdf(S_P_bins, S_P_mu, S_P_sigma), mlab.normpdf(T_P_bins, T_P_mu, T_P_sigma)</span><br><span class="line">S_N_y, T_N_y = mlab.normpdf(S_N_bins, S_N_mu, S_N_sigma), mlab.normpdf(T_N_bins, T_N_mu, T_N_sigma)</span><br><span class="line"></span><br><span class="line">plt.plot(S_P_bins, S_P_y/<span class="number">10</span>, <span class="string">'r+'</span>, alpha = <span class="number">0.4</span>)</span><br><span class="line">plt.plot(S_N_bins, S_N_y/<span class="number">10</span>, <span class="string">'rx'</span>, alpha = <span class="number">0.4</span>)</span><br><span class="line">plt.plot(T_P_bins, T_P_y/<span class="number">10</span>, <span class="string">'b+'</span>, alpha = <span class="number">0.4</span>)</span><br><span class="line">plt.plot(T_N_bins, T_N_y/<span class="number">10</span>, <span class="string">'bx'</span>, alpha = <span class="number">0.4</span>)</span><br><span class="line">plt.legend([<span class="string">'Source'</span>, <span class="string">'Target'</span>, <span class="string">'Pos. Source'</span>, <span class="string">'Neg. Source'</span>, <span class="string">'Pos. Target'</span>, <span class="string">'Neg. Target'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://img.chsong.live/Blogs/TCA/3.png-s" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">All_X = All_X.reshape(<span class="number">-1</span>, n)</span><br><span class="line">All_X /= np.linalg.norm(All_X, axis=<span class="number">0</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Gaussian Kernel</span></span><br><span class="line"><span class="string">X: mxn</span></span><br><span class="line"><span class="string">m: num of features</span></span><br><span class="line"><span class="string">n: num of instances</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(kernel_type, X, gamma = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> kernel_type == <span class="string">'linear'</span>:</span><br><span class="line">        K = np.dot(X.T, X)</span><br><span class="line">    <span class="keyword">elif</span> kernel_type == <span class="string">'rbf'</span>:</span><br><span class="line">        D = np.sum(X.T**<span class="number">2</span>, axis=<span class="number">1</span>).reshape(n,<span class="number">-1</span>).dot(np.ones((<span class="number">1</span>,n))) + \</span><br><span class="line">            np.ones((n, <span class="number">1</span>)).dot(np.sum(X.T**<span class="number">2</span>, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>,n)) - <span class="number">2</span>*X.T.dot(X)</span><br><span class="line">        K = np.exp(-gamma * D)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"></span><br><span class="line"><span class="string">'''Construct Kernel Matrix K of All_X'''</span></span><br><span class="line">K = kernel(<span class="string">'rbf'</span>, All_X, gamma = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''Construct Matrix L'''</span></span><br><span class="line">e = np.concatenate((np.ones(ns)/ns, -np.ones(nt)/nt)).reshape(n,<span class="number">-1</span>)</span><br><span class="line">L = e.dot(e.T)</span><br><span class="line">L /= np.linalg.norm(L)</span><br><span class="line"></span><br><span class="line"><span class="string">'''Construct the Matrix that need to be decomposed'''</span></span><br><span class="line">lamda = <span class="number">1</span></span><br><span class="line">M = np.linalg.pinv(K.dot(L).dot(K.T) + lamda * np.eye(n)).dot(K).dot(H).dot(K.T)</span><br><span class="line"></span><br><span class="line">eig_values, eig_vectors = np.linalg.eig(M)</span><br><span class="line">ind = np.argsort(eig_values)</span><br><span class="line">A = eig_vectors[:,ind[<span class="number">-1</span>]].reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code class="python">All_X_TCA = A.T.dot(K).squeeze()

plt.figure()
S_n, S_bins, S_patches = plt.hist(All_X_TCA[<span class="number">0</span>:<span class="number">200</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
T_n, T_bins, T_patches = plt.hist(All_X_TCA[<span class="number">200</span>: ], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
plt.close()

plt.figure()
plt.title(<span class="string">'Transfer Component Analysis'</span>)
plt.xticks([])
plt.yticks([])
plt.ylabel(<span class="string">'PDF'</span>)
plt.xlabel(<span class="string">'x'</span>)
S_mu, S_sigma = All_X_TCA[<span class="number">0</span>:<span class="number">200</span>].mean(), All_X_TCA[<span class="number">0</span>:<span class="number">200</span>].std()
T_mu, T_sigma = All_X_TCA[<span class="number">200</span>: ].mean(), All_X_TCA[<span class="number">200</span>: ].std()
S_y, T_y = mlab.normpdf(S_bins, S_mu, S_sigma), mlab.normpdf(T_bins, T_mu, T_sigma)

plt.plot(S_bins, S_y, <span class="string">'r-'</span>, alpha = <span class="number">0.5</span>)
plt.plot(T_bins, T_y, <span class="string">'b-'</span>, alpha = <span class="number">0.5</span>)

<span class="comment">##################两个领域，正负类别数据的分布</span>
plt.figure()
S_P_n, S_P_bins, S_P_patches = plt.hist(All_X_TCA[<span class="number">0</span>:<span class="number">100</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
S_N_n, S_N_bins, S_N_patches = plt.hist(All_X_TCA[<span class="number">100</span>:<span class="number">200</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
T_P_n, T_P_bins, T_P_patches = plt.hist(All_X_TCA[<span class="number">200</span>:<span class="number">300</span>], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
T_N_n, T_N_bins, T_N_patches = plt.hist(All_X_TCA[<span class="number">300</span>: ], bins=<span class="number">50</span>, alpha = <span class="number">0.5</span>)
plt.close()

S_P_mu, S_P_sigma = All_X_TCA[<span class="number">0</span>:<span class="number">100</span>].mean(), All_X_TCA[<span class="number">0</span>:<span class="number">100</span>].std()
S_N_mu, S_N_sigma = All_X_TCA[<span class="number">100</span>:<span class="number">200</span>].mean(), All_X_TCA[<span class="number">100</span>:<span class="number">200</span>].std()
T_P_mu, T_P_sigma = All_X_TCA[<span class="number">200</span>:<span class="number">300</span>].mean(), All_X_TCA[<span class="number">200</span>:<span class="number">300</span>].std()
T_N_mu, T_N_sigma = All_X_TCA[<span class="number">300</span>: ].mean(), All_X_TCA[<span class="number">300</span>: ].std()
S_P_y, T_P_y = mlab.normpdf(S_P_bins, S_P_mu, S_P_sigma), mlab.normpdf(T_P_bins, T_P_mu, T_P_sigma)
S_N_y, T_N_y = mlab.normpdf(S_N_bins, S_N_mu, S_N_sigma), mlab.normpdf(T_N_bins, T_N_mu, T_N_sigma)

plt.plot(S_P_bins, S_P_y/<span class="number">10</span>, <span class="string">'r+'</span>, alpha = <span class="number">0.4</span>)
plt.plot(S_N_bins, S_N_y/<span class="number">10</span>, <span class="string">'rx'</span>, alpha = <span class="number">0.4</span>)
plt.plot(T_P_bins, T_P_y/<span class="number">10</span>, <span class="string">'b+'</span>, alpha = <span class="number">0.4</span>)
plt.plot(T_N_bins, T_N_y/<span class="number">10</span>, <span class="string">'bx'</span>, alpha = <span class="number">0.4</span>)
plt.legend([<span class="string">'Source'</span>, <span class="string">'Target'</span>, <span class="string">'Pos. Source'</span>, <span class="string">'Neg. Source'</span>, <span class="string">'Pos. Target'</span>, <span class="string">'Neg. Target'</span>])
plt.show()</code></pre>
<p><img src="http://img.chsong.live/Blogs/TCA/4.png-s" alt="png"></p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>变长循环神经网络 [Pytorch]</title>
    <url>/20201205_%E5%8F%98%E9%95%BF%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html</url>
    <content><![CDATA[<p><img src="https://img.chsong.live/Blogs/%E5%8F%98%E9%95%BF%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png-s" alt="LSTM"><br>在使用循环神经网络时，经常碰到可变长数据，就是每一个样本的时间步是不一样的。这里总结一下pytorch里面的处理方法。</p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''导入相关包'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> rnn_utils</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">人工给出一小部分数据做示例</span></span><br><span class="line"><span class="string">这里，一共包含7个数据样本，每个样本的长度不一样，分别是:7,6,5,...,1</span></span><br><span class="line"><span class="string">数据的维度为1维</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">train_x = [torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">6</span>, <span class="number">6</span>]),</span><br><span class="line">           torch.Tensor([<span class="number">7</span>])]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">长度不一样，在代码中没法处理，因此必须将短的数据增长到与最长的数据长度一样</span></span><br><span class="line"><span class="string">一般都是补0，在pytorch里面有专门的函数: pad_sequence</span></span><br><span class="line"><span class="string">这样一来，数据将变为：</span></span><br><span class="line"><span class="string">[torch.Tensor([1, 1, 1, 1, 1, 1, 1]),</span></span><br><span class="line"><span class="string"> torch.Tensor([2, 2, 2, 2, 2, 2, 0]),</span></span><br><span class="line"><span class="string"> torch.Tensor([3, 3, 3, 3, 3, 0, 0]),</span></span><br><span class="line"><span class="string"> torch.Tensor([4, 4, 4, 4, 0, 0, 0]),</span></span><br><span class="line"><span class="string"> torch.Tensor([5, 5, 5, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string"> torch.Tensor([6, 6, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string"> torch.Tensor([7, 0, 0, 0, 0, 0, 0])]</span></span><br><span class="line"><span class="string"> 这里我们只是给出例子，看看pad_sequence的作用，为了能够训练模型，需要将其封装到数据集Dataset里面，方便DataLoader</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">x = rnn_utils.pad_sequence(train_x, batch_first=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.],
        [ 2.,  2.,  2.,  2.,  2.,  2.,  0.],
        [ 3.,  3.,  3.,  3.,  3.,  0.,  0.],
        [ 4.,  4.,  4.,  4.,  0.,  0.,  0.],
        [ 5.,  5.,  5.,  0.,  0.,  0.,  0.],
        [ 6.,  6.,  0.,  0.,  0.,  0.,  0.],
        [ 7.,  0.,  0.,  0.,  0.,  0.,  0.]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">封装到数据集MyData</span></span><br><span class="line"><span class="string">实际上是定义了一个collate_fn函数，它是用来控制在DataLoader中，load数据的时候，返回数据的格式</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyData</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_seq)</span>:</span></span><br><span class="line">        self.data_seq = data_seq</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data_seq)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data_seq[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(data)</span>:</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: len(x), reverse=<span class="literal">True</span>) <span class="comment">#将数据按数据长度从大到小排列</span></span><br><span class="line">    data_length = [len(sq) <span class="keyword">for</span> sq <span class="keyword">in</span> data]</span><br><span class="line">    data = rnn_utils.pad_sequence(data, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data.unsqueeze(<span class="number">-1</span>), data_length <span class="comment">#这里的unsqueeze是为了将数据从7x7变为7x7x1，符合模型的输入数据格式，共7个样本，维度为1，pad之后的"长度"为7</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">测试</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    data = MyData(train_x) <span class="comment">#将原始数据通过MyData封装</span></span><br><span class="line">    </span><br><span class="line">    data_loader = DataLoader(data, batch_size=<span class="number">3</span>, shuffle=<span class="literal">True</span>,collate_fn=collate_fn) <span class="comment">#封装进DataLoader，通过collate_fn控制返回数据的格式，即对每一个样本都进行pad</span></span><br><span class="line">    </span><br><span class="line">    batch_x, batch_x_len = iter(data_loader).next()</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    这里batch_size=3，因此batch_x的格式为：</span></span><br><span class="line"><span class="string">    [[[ 1.],  [1.],  [1.],  [1.],  [1.],  [1.],  [1.]],</span></span><br><span class="line"><span class="string">     [[ 3.],  [3.],  [3.],  [3.],  [3.],  [0.],  [0.]],</span></span><br><span class="line"><span class="string">     [[ 6.],  [6.],  [0.],  [0.],  [0.],  [0.],  [0.]]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    为了不让后面的0参与运算，即运算完所有的非0数据后就停止</span></span><br><span class="line"><span class="string">    pytorch里面需要对batch_x进行pack，即压缩</span></span><br><span class="line"><span class="string">    压缩后的数据格式，以及为设么这么压缩，和pytorch里面循环神经网络的工作原理有关，这里不做详细介绍</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    batch_x_pack = rnn_utils.pack_padded_sequence(batch_x, batch_x_len, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    构建模型，初始化h0,c0</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    net = nn.LSTM(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    h0 = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">    c0 = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''通过LSTM计算'''</span></span><br><span class="line">    out, (h1, c1) = net(batch_x_pack, (h0, c0))</span><br><span class="line">    </span><br><span class="line">    <span class="string">'''这里，模型输出的数据和输入的数据格式一样，都是被压缩过的，这里需要将其还原成正常的矩阵形式，使用pad_packed_sequence函数'''</span></span><br><span class="line">    out_pad, out_len = rnn_utils.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    print(out_pad)</span><br><span class="line">    print(<span class="string">'END'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0.0490,  0.1100,  0.0750,  0.0998,  0.3622, -0.0891,  0.1562, 0.0122, -0.1227,  0.0537],
         [ 0.0113,  0.0290,  0.0375,  0.1051,  0.2836, -0.1209,  0.1458, -0.0848, -0.1296, -0.0788],
         [ 0.0048, -0.0075,  0.0173,  0.0673,  0.2713, -0.1431,  0.1392, -0.1227, -0.1214, -0.1425],
         [ 0.0102, -0.0246,  0.0090,  0.0291,  0.2600, -0.1579,  0.1342, -0.1392, -0.1135, -0.1720],
         [ 0.0177, -0.0328,  0.0058, -0.0016,  0.2524, -0.1678,  0.1313, -0.1480, -0.1091, -0.1863],
         [ 0.0240, -0.0371,  0.0048, -0.0244,  0.2475, -0.1742,  0.1300, -0.1542, -0.1074, -0.1939],
         [ 0.0286, -0.0398,  0.0047, -0.0408,  0.2445, -0.1782,  0.1298, -0.1592, -0.1071, -0.1983]],

        [[ 0.2161, -0.0114,  0.1041,  0.0859,  0.0486, -0.0040,  0.2124, 0.1901,  0.1633,  0.1281],
         [ 0.1089, -0.0357,  0.0969,  0.0758,  0.1588, -0.1085,  0.1722, 0.0266, -0.0036, -0.0442],
         [ 0.0438, -0.0525,  0.0566,  0.0414,  0.2152, -0.1581,  0.1527, -0.0615, -0.0681, -0.1262],
         [ 0.0199, -0.0631,  0.0303,  0.0025,  0.2376, -0.1802,  0.1418, -0.1058, -0.0890, -0.1643],
         [ 0.0137, -0.0696,  0.0154, -0.0301,  0.2450, -0.1906,  0.1362, -0.1309, -0.0954, -0.1821],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000]],

        [[-0.0518,  0.0285,  0.1449,  0.1145,  0.2673, -0.0603,  0.1157, .1086, -0.0482, -0.0614],
         [-0.0578, -0.0089,  0.0664,  0.0769,  0.2777, -0.1151,  0.1328, -0.0094, -0.0666, -0.1451],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 0.0000,  0.0000,  0.0000]]])
END</code></pre>]]></content>
      <tags>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>Joint Distribution Adaptation (JDA)</title>
    <url>/20201205_%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0_JDA/index.html</url>
    <content><![CDATA[<p><img src="https://img.chsong.live/Blogs/JDA/1.png-s" alt="JDA"></p>
<p>JDA方法首次发表于2013年的ICCV（计算机视觉领域顶会，与CVPR类似），它的作者是清华大学的博士生（现为清华大学助理教授）龙明盛。联合分布适配方法（joint distribution adaptation,JDA）解决的也是迁移学习中一类很大的问题：domain adaptation。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> sklearn.metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.linalg</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<p>数据集是我自己处理好的手写数字识别数据集，关于数据集的信息可以在<a href="https://tianchi.aliyun.com/notebook-ai/home#datasetLabId=48612&operaType=2" target="_blank" rel="noopener">此处</a>查看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">usps = scio.loadmat(<span class="string">'datalab/48612/usps.mat.bin'</span>)</span><br><span class="line">mnist = scio.loadmat(<span class="string">'datalab/48612/mnist.mat.bin'</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">'''Visualization'''</span></span><br><span class="line"><span class="comment">## images in MNIST</span></span><br><span class="line">mnist_rdm = np.random.randint(<span class="number">2000</span>, size=<span class="number">16</span>)</span><br><span class="line">mnist_labels = mnist[<span class="string">'Y'</span>][mnist_rdm].reshape(<span class="number">4</span>,<span class="number">4</span>).T - <span class="number">1</span></span><br><span class="line">mnist_ims = mnist[<span class="string">'X'</span>][:,mnist_rdm].T</span><br><span class="line">mnist_ims = np.array([im.reshape(<span class="number">16</span>,<span class="number">16</span>) <span class="keyword">for</span> im <span class="keyword">in</span> mnist_ims])</span><br><span class="line">mnist_ims = functools.reduce(<span class="keyword">lambda</span> x, y : np.concatenate((x, y),axis=<span class="number">1</span>), np.array([mnist_ims[i*<span class="number">4</span>:(i+<span class="number">1</span>)*<span class="number">4</span>].reshape(<span class="number">4</span>*<span class="number">16</span>,<span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]))</span><br><span class="line">print(mnist_labels)</span><br><span class="line">plt.imshow(mnist_ims, cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">## images in USPS</span></span><br><span class="line">usps_rdm = np.random.randint(<span class="number">1800</span>, size=<span class="number">16</span>)</span><br><span class="line">usps_labels = usps[<span class="string">'Y'</span>][usps_rdm].reshape(<span class="number">4</span>,<span class="number">4</span>).T - <span class="number">1</span></span><br><span class="line">usps_ims = usps[<span class="string">'X'</span>][:,usps_rdm].T</span><br><span class="line">usps_ims = np.array([im.reshape(<span class="number">16</span>,<span class="number">16</span>) <span class="keyword">for</span> im <span class="keyword">in</span> usps_ims])</span><br><span class="line">usps_ims = functools.reduce(<span class="keyword">lambda</span> x, y : np.concatenate((x, y),axis=<span class="number">1</span>), np.array([usps_ims[i*<span class="number">4</span>:(i+<span class="number">1</span>)*<span class="number">4</span>].reshape(<span class="number">4</span>*<span class="number">16</span>,<span class="number">-1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)]))</span><br><span class="line">print(usps_labels)</span><br><span class="line">plt.imshow(usps_ims, cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>[[2 8 7 9]
 [1 3 3 1]
 [5 4 8 2]
 [2 2 4 4]]</code></pre><p><img src="https://img.chsong.live/Blogs/JDA/2.png-s" alt="png"></p>
<pre><code>[[1 4 0 0]
 [2 0 0 4]
 [5 2 7 0]
 [9 2 8 3]]</code></pre><p><img src="https://img.chsong.live/Blogs/JDA/3.png-s" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Kernels</span></span><br><span class="line"><span class="string">X: m x n</span></span><br><span class="line"><span class="string">m: num of features</span></span><br><span class="line"><span class="string">n: num of instances</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel</span><span class="params">(kernel_type, X, gamma = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> kernel_type == <span class="string">'original'</span>:</span><br><span class="line">        K = X</span><br><span class="line">    <span class="keyword">elif</span> kernel_type == <span class="string">'linear'</span>:</span><br><span class="line">        K = np.dot(X.T, X)</span><br><span class="line">    <span class="keyword">elif</span> kernel_type == <span class="string">'rbf'</span>:</span><br><span class="line">        D = np.sum(X.T**<span class="number">2</span>, axis=<span class="number">1</span>).reshape(X.shape[<span class="number">1</span>],<span class="number">-1</span>).dot(np.ones((<span class="number">1</span>,X.shape[<span class="number">1</span>]))) + \</span><br><span class="line">            np.ones((X.shape[<span class="number">1</span>], <span class="number">1</span>)).dot(np.sum(X.T**<span class="number">2</span>, axis=<span class="number">1</span>).reshape(<span class="number">-1</span>,X.shape[<span class="number">1</span>])) - <span class="number">2</span>*X.T.dot(X)</span><br><span class="line">        K = np.exp(-gamma * D)</span><br><span class="line">    <span class="keyword">return</span> K</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">JDA</span><span class="params">(source, target, lamda, gamma, kernel_type=<span class="string">'rbf'</span>, iterations=<span class="number">10</span>, Y_pseudo=None)</span>:</span></span><br><span class="line">    X_source, Y_source = source[<span class="string">'X'</span>], source[<span class="string">'Y'</span>]</span><br><span class="line">    X_target, Y_target = target[<span class="string">'X'</span>], target[<span class="string">'Y'</span>]</span><br><span class="line">    ns, nt = len(Y_source), len(Y_target)</span><br><span class="line">    n = ns + nt</span><br><span class="line">    <span class="string">'''concatenate the source and target data'''</span></span><br><span class="line">    X = np.concatenate((X_source, X_target), axis = <span class="number">1</span>)</span><br><span class="line">    <span class="string">'''normlization is important, the acc is very low without this step'''</span></span><br><span class="line">    X /= np.linalg.norm(X, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="string">'''construct the matrix H'''</span></span><br><span class="line">    H = np.eye(n) - (<span class="number">1</span>/n) * np.ones((n,n))</span><br><span class="line">    <span class="string">'''construct the matrix M0'''</span></span><br><span class="line">    e0 = np.concatenate((np.ones((ns,<span class="number">1</span>))/ns, -np.ones((nt,<span class="number">1</span>))/nt), axis=<span class="number">0</span>)</span><br><span class="line">    M0 = e0.dot(e0.T) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> Y_pseudo <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        C = len(np.unique(Y_source))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">            print(<span class="string">'    iteration:'</span>,i+<span class="number">1</span>, <span class="string">' '</span>, end=<span class="string">''</span>)</span><br><span class="line">            N = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, C+<span class="number">1</span>):</span><br><span class="line">                e = np.zeros((n, <span class="number">1</span>))</span><br><span class="line">                e[np.where(Y_source==c)] = <span class="number">1</span> / len(Y_source[np.where(Y_source==c)])</span><br><span class="line">                e[np.where(Y_pseudo==c)[<span class="number">0</span>] + ns] = <span class="number">-1</span> / len(Y_pseudo[np.where(Y_pseudo==c)])</span><br><span class="line">                N += e.dot(e.T)</span><br><span class="line"></span><br><span class="line">            M = M0 + N</span><br><span class="line">            M /= np.linalg.norm(M,<span class="string">'fro'</span>)</span><br><span class="line">            n_eye = X.shape[<span class="number">0</span>] <span class="keyword">if</span> kernel_type == <span class="string">'original'</span> <span class="keyword">else</span> n</span><br><span class="line">            K = kernel(kernel_type, X, gamma)</span><br><span class="line">            eig_vals, eig_vecs = np.linalg.eig(np.linalg.pinv(np.linalg.multi_dot([K, H, K.T])).dot(np.linalg.multi_dot([K, M, K.T]) + lamda * np.eye(n_eye)))</span><br><span class="line">            ind = np.argsort(eig_vals)</span><br><span class="line">            A = eig_vecs[:,ind[:<span class="number">100</span>]]</span><br><span class="line">            Z = A.T.dot(K)</span><br><span class="line">            Z /= np.linalg.norm(Z, axis = <span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            X_source_JDA, X_target_JDA = Z[:,:ns], Z[:,ns:]</span><br><span class="line">            </span><br><span class="line">            pseudo = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">            pseudo.fit(X_source_JDA.T, Y_source.flatten())</span><br><span class="line">            Y_pseudo = pseudo.predict(X_target_JDA.T)</span><br><span class="line">            pseudo_acc = sklearn.metrics.accuracy_score(Y_target.flatten(), Y_pseudo)</span><br><span class="line">            print(<span class="string">'acc:'</span>,pseudo_acc)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> X_source_JDA, X_target_JDA</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### '''两个领域，分别是mnist, usps. 可以在增加其他数据集，构造多个领域'''</span></span><br><span class="line"><span class="string">'''各个领域分别作为源领域和目标领域，'''</span></span><br><span class="line">domain_names = [<span class="string">'mnist'</span>, <span class="string">'usps'</span>]</span><br><span class="line">domains = [mnist, usps]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(domains)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(domains)):</span><br><span class="line">        <span class="keyword">if</span> i != j:</span><br><span class="line">            print(<span class="string">'\n from'</span>, domain_names[i], <span class="string">'to'</span>, domain_names[j])</span><br><span class="line">            </span><br><span class="line">            source, target = domains[i], domains[j]</span><br><span class="line">            baseline = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">            baseline.fit(source[<span class="string">'X'</span>].T, source[<span class="string">'Y'</span>].flatten())</span><br><span class="line">            Y_pseudo_target = baseline.predict(target[<span class="string">'X'</span>].T)</span><br><span class="line">            baseline_acc = sklearn.metrics.accuracy_score(target[<span class="string">'Y'</span>].flatten(), Y_pseudo_target)</span><br><span class="line">            print(<span class="string">'    acc of baseline 1-NN:'</span>, baseline_acc)</span><br><span class="line">            </span><br><span class="line">            X_JDA_source, X_JDA_target = JDA(source, target, lamda=<span class="number">1</span>, gamma=<span class="number">1</span>, kernel_type=<span class="string">'rbf'</span>, iterations=<span class="number">5</span>, Y_pseudo=Y_pseudo_target)</span><br><span class="line">            jda = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">            jda.fit(X_JDA_source.T, source[<span class="string">'Y'</span>].flatten())</span><br><span class="line">            Y_pseudo_target = jda.predict(X_JDA_target.T)</span><br><span class="line">            jda_acc = sklearn.metrics.accuracy_score(target[<span class="string">'Y'</span>].flatten(), Y_pseudo_target)</span><br><span class="line">            print(<span class="string">'    acc of jda:'</span>, jda_acc)</span><br></pre></td></tr></table></figure>
<p>from mnist to usps<br>    acc of baseline 1-NN: 0.644444444444<br>    iteration: 1  acc: 0.743333333333<br>    iteration: 2  acc: 0.764444444444<br>    iteration: 3  acc: 0.756666666667<br>    iteration: 4  acc: 0.755555555556<br>    iteration: 5  acc: 0.757777777778<br>    acc of jda: 0.757777777778</p>
<p>from usps to mnist<br>    acc of baseline 1-NN: 0.3585<br>    iteration: 1  acc: 0.583<br>    iteration: 2  acc: 0.6165<br>    iteration: 3  acc: 0.613<br>    iteration: 4  acc: 0.6135<br>    iteration: 5  acc: 0.619<br>    acc of jda: 0.619</p>
<p>效果比JDA原文里面的实验结果好很多，尤其是从mnist到usps，可能是因为数据的原因，虽然都是随即抽取的样本，mnist：2000，usps：1800. 跟原文还是有一些差别。</p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>五杀片段</title>
    <url>/20201229_%E4%BA%94%E8%BF%9E%E7%BB%9D%E4%B8%96/index.html</url>
    <content><![CDATA[<p>首次五杀，居然是之前一直不怎么喜欢的大小姐。</p>
<p><img src="http://img.chsong.live/Blogs/%E4%BA%94%E8%BF%9E%E7%BB%9D%E4%B8%96/%E5%AD%99%E5%B0%9A%E9%A6%99.jpg-o" alt="五杀"></p>
<a id="more"></a>

<p><video src="http://chengsong-img.oss-cn-hangzhou.aliyuncs.com/Blogs/%E4%BA%94%E8%BF%9E%E7%BB%9D%E4%B8%96/%E5%AD%99%E5%B0%9A%E9%A6%99.mp4" width="800px" height="450px" controls="controls"></video> </p>
]]></content>
      <tags>
        <tag>动态</tag>
      </tags>
  </entry>
</search>
